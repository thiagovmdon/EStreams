{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46e94d6f",
   "metadata": {},
   "source": [
    "# Meteorological stations coverage extraction\n",
    "\n",
    "\n",
    "Author: Thiago Nascimento (thiago.nascimento@eawag.ch)\n",
    "\n",
    "This notebook is part of the EStreams publication and was used to extract and aggregate the weather stations coverage information from the E-OBS dataset.\n",
    "\n",
    "* Note that this code enables not only the replicability of the current database but also the extrapolation to new catchment areas. \n",
    "* Additionally, the user should download and insert the original raw-data in the folder of the same name prior to run this code. \n",
    "* The original third-party data used were not made avaialable in this repository due to redistribution and storage-space reasons.  \n",
    "\n",
    "## Requirements\n",
    "**Python:**\n",
    "\n",
    "* Python>=3.6\n",
    "* Jupyter\n",
    "* geopandas=0.10.2\n",
    "* numpy\n",
    "* os\n",
    "* pandas\n",
    "* shapely\n",
    "* tqdm\n",
    "\n",
    "Check the Github repository for an environment.yml (for conda environments) or requirements.txt (pip) file.\n",
    "\n",
    "**Files:**\n",
    "\n",
    "* data/shapefiles/estreams_catchments.shp\n",
    "* data/eobs_stations/stations_info_{rr, tg, tn, tx, pp, hu, fg, qq}_v28.0e.txt. https://www.ecad.eu/download/ensembles/download.php (Last access: 27 November 2023)\n",
    "\n",
    "**Directory:**\n",
    "\n",
    "* Clone the GitHub directory locally\n",
    "* Place any third-data variables in their respective directory.\n",
    "* ONLY update the \"PATH\" variable in the section \"Configurations\", with their relative path to the EStreams directory. \n",
    "\n",
    "## References\n",
    "* Cornes, R., G. van der Schrier, E.J.M. van den Besselaar, and P.D. Jones. 2018: An Ensemble Version of the E-OBS Temperature and Precipitation Datasets, J. Geophys. Res. Atmos., 123. doi:10.1029/2017JD028200\n",
    "\n",
    "## Licenses\n",
    "* EOBS: \"The ECA&D data policy applies. These observational data are strictly for use in non-commercial research and non-commercial education projects only. Scientific results based on these data must be submitted for publication in the open literature without any delay linked to commercial objectives\" https://www.ecad.eu/download/ensembles/download.php#guidance (Last access: 27 November 2023)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f7648ab",
   "metadata": {},
   "source": [
    "# Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c70b9bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from shapely.geometry import Point, Polygon\n",
    "import tqdm as tqdm\n",
    "from utils.hydrology import count_geometries_in_polygons"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33f0a427",
   "metadata": {},
   "source": [
    "# Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13078a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only editable variables:\n",
    "# Relative path to your local directory\n",
    "PATH = \"../../..\"\n",
    "# Do not change the order of the variables:\n",
    "filenames = ['data/eobs_stations/stations_info_qq_v28.0e.txt', 'data/eobs_stations/stations_info_pp_v28.0e.txt',\n",
    "             'data/eobs_stations/stations_info_tg_v28.0e.txt','data/eobs_stations/stations_info_tx_v28.0e.txt',\n",
    "             'data/eobs_stations/stations_info_fg_v28.0e.txt','data/eobs_stations/stations_info_rr_v28.0e.txt',\n",
    "             'data/eobs_stations/stations_info_tn_v28.0e.txt','data/eobs_stations/stations_info_hu_v28.0e.txt']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9e44727",
   "metadata": {},
   "source": [
    "* #### The users should NOT change anything in the code below here.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "914b12c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Non-editable variables:\n",
    "PATH_OUTPUT = \"results/staticattributes/\"\n",
    "# Set the directory:\n",
    "os.chdir(PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed0cecf2",
   "metadata": {},
   "source": [
    "# Import data\n",
    "## Catchment boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb3806a",
   "metadata": {},
   "outputs": [],
   "source": [
    "catchment_boundaries = gpd.read_file('data/shapefiles/estreams_catchments.shp')\n",
    "catchment_boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d200f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The total number of catchments to be processed are:\", len(catchment_boundaries))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2231a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here you can check the crs of the datasets:\n",
    "print(\"CRS of catchment_boundaries:\", catchment_boundaries.crs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c79daf2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the target CRS to ETRS89 LAEA (3035)\n",
    "target_crs = 'EPSG:3035'  \n",
    "\n",
    "# Reproject the GeoDataFrame to the target CRS\n",
    "catchment_boundaries_reprojected = catchment_boundaries.to_crs(target_crs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8f8039c",
   "metadata": {},
   "source": [
    "### E-OBS stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03cf0eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we analyse the precipitation gauges:\n",
    "filename = filenames[5]\n",
    "filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdbf809f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use read_csv with the '|' delimiter\n",
    "eobs_stations = pd.read_csv('data/eobs_stations/stations_info_rr_v28.0e.txt', delimiter='|', encoding='latin1')\n",
    "eobs_stations.columns = ['STATION', 'NAME','COUNTRY', 'LAT', 'LON', 'ELEV',\n",
    "       'START', 'STOP']\n",
    "eobs_stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82eac433",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create your dataframes\n",
    "eobs_stations_qq = pd.DataFrame()\n",
    "eobs_stations_pp = pd.DataFrame()\n",
    "eobs_stations_tg = pd.DataFrame()\n",
    "eobs_stations_tx = pd.DataFrame()\n",
    "eobs_stations_fg = pd.DataFrame()\n",
    "eobs_stations_rr = pd.DataFrame()\n",
    "eobs_stations_tn = pd.DataFrame()\n",
    "eobs_stations_hu = pd.DataFrame()\n",
    "\n",
    "# Store dataframes in a dictionary\n",
    "dataframes = {\n",
    "    'qq': eobs_stations_qq,\n",
    "    'pp': eobs_stations_pp,\n",
    "    'tg': eobs_stations_tg,\n",
    "    'tx': eobs_stations_tx,\n",
    "    'fg': eobs_stations_fg,\n",
    "    'rr': eobs_stations_rr,\n",
    "    'tn': eobs_stations_tn,\n",
    "    'hu': eobs_stations_hu\n",
    "}\n",
    "\n",
    "selected_variables = ['qq', 'pp', 'tg', 'tx', 'fg', 'rr', 'tn', 'hu']\n",
    "\n",
    "i = 0\n",
    "\n",
    "for filename in filenames:\n",
    "    # Use read_csv with the '|' delimiter\n",
    "    eobs_stations = pd.read_csv(filename, delimiter='|', encoding='latin1')\n",
    "    eobs_stations.columns = ['STATION', 'NAME','COUNTRY', 'LAT', 'LON', 'ELEV',\n",
    "       'START', 'STOP']\n",
    "    \n",
    "    dataframes[selected_variables[i]] = eobs_stations\n",
    "    \n",
    "    i = i + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb3d9f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes[selected_variables[5]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c146cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we convert the dataframes to geodataframes and set the projected system\n",
    "# We need to reproject our geodataframes to a projected coordinate system (in meters) in order to \n",
    "# provide the buffer correctly.\n",
    "\n",
    "for variable in selected_variables:\n",
    "    # Convert the DataFrame to a GeoDataFrame\n",
    "    geometry = [Point(lon, lat) for lon, lat in zip(dataframes[variable]['LON'], dataframes[variable]['LAT'])]\n",
    "    dataframes[variable] = gpd.GeoDataFrame(dataframes[variable], geometry=geometry)\n",
    "\n",
    "    # Set the coordinate reference system (CRS) for WGS-84\n",
    "    dataframes[variable].crs = 'EPSG:4326'\n",
    "    \n",
    "    # Define the target CRS to ETRS89 LAEA (3035)\n",
    "    target_crs = 'EPSG:3035'  \n",
    "\n",
    "    # Reproject the GeoDataFrame to the target CRS\n",
    "    dataframes[variable] = dataframes[variable].to_crs(target_crs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "054f4ce5",
   "metadata": {},
   "source": [
    "## Buffer of the catchments boundaries\n",
    "* This may take several minutes to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c760fd2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_catchment = catchment_boundaries_reprojected.copy()\n",
    "\n",
    "# First we make a buffer of 10 km around the catchment shapefiles \n",
    "buffer_distance = 10000\n",
    "buffered_catchment_boundaries_reprojected = subset_catchment.copy()\n",
    "buffered_catchment_boundaries_reprojected['geometry'] = subset_catchment['geometry'].buffer(buffer_distance)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4adb6050",
   "metadata": {},
   "source": [
    "## Computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6abc9456",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_variables = ['qq', 'pp', 'tg', 'tx', 'fg', 'rr', 'tn', 'hu']\n",
    "\n",
    "# First we create an empty dataframe:\n",
    "num_stations = pd.DataFrame()\n",
    "num_stations[\"area\"] = buffered_catchment_boundaries_reprojected.set_index(\"basin_id\", inplace = False).area_calc\n",
    "\n",
    "for variable in tqdm.tqdm(selected_variables):\n",
    "    \n",
    "    # Here we use utils.hydrology.count_geometries_in_polygons function\n",
    "    num_stations[\"stations_num_\"+variable] = count_geometries_in_polygons(dataframes[variable], \n",
    "                                                                 buffered_catchment_boundaries_reprojected, \"basin_id\", \n",
    "                                                                 new_column=\"num\")\n",
    "    \n",
    "    num_stations[\"stations_dens_\"+variable] = num_stations[\"stations_num_\"+variable] / num_stations[\"area\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc09819",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c19cd4ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_stations_coverage = num_stations.iloc[:, 1:]\n",
    "num_stations_coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d76ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_stations_coverage.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7e30c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of values for replacement\n",
    "old_values = ['_qq', '_pp', '_tg', '_tx', '_fg', '_rr', '_tn', '_hu']\n",
    "new_values = ['_swr_mean', '_sp_mean', '_t_mean', '_t_max', '_ws_mean', '_p_mean', '_t_min', '_rh_mean']\n",
    "\n",
    "# Create a mapping dictionary\n",
    "column_name_mapping = {old: new for old, new in zip(old_values, new_values)}\n",
    "\n",
    "# Replace the specified patterns in column names\n",
    "num_stations_coverage.columns = num_stations_coverage.columns.to_series().replace(column_name_mapping, regex=True).values\n",
    "num_stations_coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b0231b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we sort the index:\n",
    "num_stations_coverage = num_stations_coverage.sort_index(axis=0)\n",
    "num_stations_coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4371fb1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Round the data to 3 decimals\n",
    "num_stations_coverage = num_stations_coverage.astype(float).round(3)\n",
    "num_stations_coverage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69aa7a8b",
   "metadata": {},
   "source": [
    "## Data export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "731f2cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export the final analysis:\n",
    "num_stations_coverage.to_csv(PATH_OUTPUT+\"/estreams_meteorology_density.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3f9f5e7",
   "metadata": {},
   "source": [
    "# End"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
