{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1593ce95",
   "metadata": {},
   "source": [
    "# Lithology attributes extraction\n",
    "\n",
    "\n",
    "Author: Thiago Nascimento (thiago.nascimento@eawag.ch)\n",
    "\n",
    "This notebook is part of the EStreams publication and was used to extract and aggregate the lithological attributes from the GLiM shapefile to the catchment boundaries.\n",
    "\n",
    "* Note that this code enables not only the replicability of the current database but also the extrapolation to new catchment areas. \n",
    "* Additionally, the user should download and insert the original raw-data in the folder of the same name prior to run this code. \n",
    "* The original third-party data used were not made avaialable in this repository due to redistribution and storage-space reasons.  \n",
    "\n",
    "## Requirements\n",
    "**Python:**\n",
    "* Python>=3.6\n",
    "* Jupyter\n",
    "* geopandas=0.10.2\n",
    "* numpy\n",
    "* os\n",
    "* pandas\n",
    "* rasterio\n",
    "* time\n",
    "* tqdm\n",
    "\n",
    "Check the Github repository for an environment.yml (for conda environments) or requirements.txt (pip) file.\n",
    "\n",
    "**Files:**\n",
    "* data/lithology/GLiM.shp. Available at: http://dx.doi.org/10.1594/PANGAEA.788537 (Last access 23 November 2023)\n",
    "* data/lithology/average_soil_and_sedimentary-deposit_thickness.tif. Available at: https://daac.ornl.gov/cgi-bin/dsviewer.pl?ds_id=1304 (Last access 23 November 2023)\n",
    "* data/shapefiles/estreams_boundaries.shp\n",
    "\n",
    "**Directory:**\n",
    "\n",
    "* Clone the GitHub directory locally\n",
    "* Place any third-data variables in their respective directory.\n",
    "* ONLY update the \"PATH\" variable in the section \"Configurations\", with their relative path to the EStreams directory. \n",
    "\n",
    "## References\n",
    "\n",
    "* Hartmann, J., and Moosdorf, N. (2012), The new global lithological map database GLiM: A representation of rock properties at the Earth surface, Geochem. Geophys. Geosyst., 13, Q12004, https://doi.org/10.1029/2012GC004370\n",
    "\n",
    "* Pelletier, J.D., P.D. Broxton, P. Hazenberg, X. Zeng, P.A. Troch, G. Niu, Z.C. Williams, M.A. Brunke, and D. Gochis. 2016. Global 1-km Gridded Thickness of Soil, Regolith, and Sedimentary Deposit Layers. ORNL DAAC, Oak Ridge, Tennessee, USA. https://doi.org/10.3334/ORNLDAAC/1304\n",
    "\n",
    "\n",
    "## Licenses\n",
    "\n",
    "* GLiM: Creative Commons Attribution 3.0 Unported (CC-BY-3.0). https://daac.ornl.gov/cgi-bin/dsviewer.pl?ds_id=1304 (Last access 27 November 2023)\n",
    "* Depth to bedrock: Open-access. https://daac.ornl.gov/cgi-bin/dsviewer.pl?ds_id=1304 (Last access 27 November 2023)\n",
    "\n",
    "\n",
    "## Observations\n",
    "\n",
    "#### GLiM lithological classes\n",
    "\n",
    "1. nd: No Data\n",
    "2. su: Unconsolidated Sediments\n",
    "3. ss: Siliciclastic Sedimentary \n",
    "4. sm: Mixed Sedimentary Rocks\n",
    "5. sc: Carbonate Sedimentary Rocks\n",
    "6. py: Pyroclastics\n",
    "7. ev: Evaporites\n",
    "8. mt: Metamorphic Rocks\n",
    "9. pa: Acid Plutonic Rocks\n",
    "10. pi: Intermediate Plutonic Rocks\n",
    "11. pb: Basic Plutonic Rocks\n",
    "12. va: Acid Volcanic Rocks\n",
    "13. vi: Intermediate Vulcanic Rocks\n",
    "14. vb: Basic Volcanic Rocks\n",
    "15. ig: Ice and Glaciers\n",
    "16. wb: Water bodies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f7648ab",
   "metadata": {},
   "source": [
    "# Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f6e053",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import tqdm as tqdm\n",
    "import os\n",
    "import numpy as np\n",
    "import rasterio\n",
    "import time\n",
    "from rasterio.features import geometry_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b31c9d67",
   "metadata": {},
   "source": [
    "# Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e0b44e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only editable variables:\n",
    "# Relative path to your local directory\n",
    "PATH = \"../../..\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ca5877e",
   "metadata": {},
   "source": [
    "* #### The users should NOT change anything in the code below here.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13078a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Non-editable variables\n",
    "PATH = \"../../..\"\n",
    "PATH_OUTPUT = \"results/staticattributes/\"\n",
    "# Set the directory:\n",
    "os.chdir(PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4909fe84",
   "metadata": {},
   "source": [
    "# Import data\n",
    "## Catchment boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5948d5bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "catchment_boundaries = gpd.read_file('data/shapefiles/estreams_catchments.shp')\n",
    "catchment_boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e1d16e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The total number of catchments to be processed are:\", len(catchment_boundaries))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "761ff848",
   "metadata": {},
   "source": [
    "## Depth to bedrock raster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f47633e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the path to the raster file you want to open\n",
    "raster_depthtobedrock = \"data/lithology/average_soil_and_sedimentary-deposit_thickness.tif\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fa57cc5",
   "metadata": {},
   "source": [
    "## GLiM shapefile"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "439c2e11",
   "metadata": {},
   "source": [
    "GLiM original layer. Note that you can read an already clipped version and speed-up the processing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a81e2a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "GLiM = gpd.read_file('data/lithology/GLiM.shp')\n",
    "GLiM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a06bdadd",
   "metadata": {},
   "source": [
    "To optimize the process it is important to dissolve the polygon geometries before intersecting the areas. Here we dissove it by the atribute field corresponding to the unique-id for each lithological class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d95351fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "attribute_field = 'xx'\n",
    "GLiM_dissolved = GLiM.dissolve(by=attribute_field)\n",
    "\n",
    "# Now we create a new feature with the lithology class:\n",
    "GLiM_dissolved[\"class\"] = GLiM_dissolved.index\n",
    "GLiM_dissolved"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8491031",
   "metadata": {},
   "source": [
    "## Reproject to projected coordinates system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b01e3bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here you can check the crs of the datasets:\n",
    "print(\"CRS of catchment_boundaries:\", catchment_boundaries.crs)\n",
    "print(\"CRS of GLiM:\", GLiM_dissolved.crs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be378c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the target CRS to ETRS89 LAEA (3035)\n",
    "target_crs = 'EPSG:3035'  \n",
    "\n",
    "# Reproject the GeoDataFrame to the target CRS\n",
    "catchment_boundaries_reprojected = catchment_boundaries.to_crs(target_crs)\n",
    "GLiM_reprojected = GLiM_dissolved.to_crs(target_crs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5677474",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here you can check the crs of the datasets:\n",
    "print(\"CRS of catchment_boundaries:\", catchment_boundaries_reprojected.crs)\n",
    "print(\"CRS of GLiM:\", GLiM_reprojected.crs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83fa96f9",
   "metadata": {},
   "source": [
    "# Bedrock depth extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b1ad54c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the bedrock we use the crs in 4326:\n",
    "subset_catchment=catchment_boundaries.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59467f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create lists to store the results\n",
    "avg_values = []\n",
    "with rasterio.open(raster_depthtobedrock) as src:\n",
    "    for idx, geom in tqdm.tqdm(subset_catchment.iterrows()):\n",
    "        # Create a mask for the geometry\n",
    "        mask = geometry_mask([geom['geometry']], out_shape=src.shape, transform=src.transform, invert=True)\n",
    "\n",
    "        # Read the values within the geometry from the raster\n",
    "        values = src.read(1, masked=True)\n",
    "        values = values[mask]\n",
    "\n",
    "        # Calculate statistics only if there are valid values in the 'values' array\n",
    "        if len(values) > 0:\n",
    "            avg_value = np.mean(values)\n",
    "\n",
    "        else:\n",
    "            # Handle the case when there are no valid values (e.g., by setting them to NaN or a specific value)\n",
    "            avg_value = np.nan\n",
    "\n",
    "        # Store the results in the lists\n",
    "        avg_values.append(avg_value)\n",
    "\n",
    "# Create a DataFrame to store the results for this file\n",
    "data = {\n",
    "    'basin_id': subset_catchment['basin_id'],\n",
    "    'bedrk_dep': avg_values,\n",
    "}\n",
    "bedrk_dep_df = pd.DataFrame(data)\n",
    "bedrk_dep_df.set_index(\"basin_id\", inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6ec4ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "bedrk_dep_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2d52822",
   "metadata": {},
   "source": [
    "# Intersection areas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ae54e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_catchment=catchment_boundaries_reprojected.copy()\n",
    "subset_catchment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "333138d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Record the start time\n",
    "start_time = time.time()\n",
    "\n",
    "lithology_overlap = gpd.overlay(df1=subset_catchment, df2=GLiM_reprojected, how='intersection')\n",
    "\n",
    "# Record the end time\n",
    "end_time = time.time()\n",
    "\n",
    "# Print the elapsed time in seconds when done:\n",
    "print(\"Elapsed time: {:.1f} seconds\".format(end_time - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30baae50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the areas of the overlapping polygons and add them as a new column\n",
    "lithology_overlap['area_sqm'] = lithology_overlap['geometry'].area/1000000\n",
    "lithology_overlap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d417dc1",
   "metadata": {},
   "source": [
    "# Pivot table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a0b283",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally we can creatre a pivot-table with the percentage of each lithological class per catchment:\n",
    "\n",
    "lithology_areas = pd.pivot_table(\n",
    "    lithology_overlap,\n",
    "    values='area_sqm',     # Replace with the actual column name for the area\n",
    "    index='basin_id',      # Rows are based on 'basin_id'\n",
    "    columns='class',       # Columns are based on 'class' (the class)\n",
    "    aggfunc='sum',         # Sum the areas for each combination\n",
    "    fill_value=0           # Replace NaN with 0\n",
    ")\n",
    "\n",
    "# Here we can sum to compute the total area of each catchment: \n",
    "lithology_areas.loc[:, \"totalarea\"] = lithology_areas.sum(axis = 1)\n",
    "lithology_areas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2bb7250",
   "metadata": {},
   "source": [
    "## Catchment covered by shapefile\n",
    "* Here we compute the total catchment area covered by the lithology shapefile:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83401c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "catchment_boundaries_reprojected.set_index('basin_id', inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f3e9581",
   "metadata": {},
   "outputs": [],
   "source": [
    "lithology_areas['area_calc'] = catchment_boundaries_reprojected.area / 1000000\n",
    "lithology_areas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e72e1f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "lithology_areas['tot_area'] = lithology_areas.totalarea / lithology_areas.area_calc\n",
    "lithology_areas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0a40490",
   "metadata": {},
   "source": [
    "# Data organization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d064091c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we compute the lithology percentages from each class:\n",
    "lithology_df = (lithology_areas.iloc[:, :16].div(lithology_areas['totalarea'], axis=0))*100\n",
    "lithology_df = lithology_df.iloc[:, 0:-1]\n",
    "lithology_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca351d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new column with the name of the column with the majority class\n",
    "lithology_df['lit_dom'] = lithology_df.apply(lambda row: row.idxmax(), axis=1)\n",
    "\n",
    "# Add \"th_new_\" as a prefix to all column names\n",
    "lithology_df = lithology_df.add_prefix('lit_fra_')\n",
    "lithology_df = lithology_df.rename(columns={'lit_fra_lit_dom': 'lit_dom'})\n",
    "\n",
    "# Catchment ara covered by the lithology shapef\n",
    "lithology_df['tot_area'] = (lithology_areas.tot_area)*100\n",
    "\n",
    "# Concatenate the bedrock depth:\n",
    "lithology_df[\"bedrk_dep\"] = bedrk_dep_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5abcd780",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Here we sort the index:\n",
    "lithology_df = lithology_df.sort_index(axis=0)\n",
    "lithology_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a2f1fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Round the data to 3 decimals:\n",
    "lithology_df.iloc[:, 0:-3] = lithology_df.iloc[:, 0:-3].round(3)\n",
    "lithology_df.iloc[:, -2:] = lithology_df.iloc[:, -2:].round(3)\n",
    "\n",
    "lithology_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "790b61f8",
   "metadata": {},
   "source": [
    "# Data export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce35b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export the final dataset:\n",
    "lithology_df.to_csv(PATH_OUTPUT+\"estreams_geology_attributes.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cfa5cd2",
   "metadata": {},
   "source": [
    "# End"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
