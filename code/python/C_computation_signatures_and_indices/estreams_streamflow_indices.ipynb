{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46e94d6f",
   "metadata": {},
   "source": [
    "# Streamflow indices computation\n",
    "\n",
    "Author: Thiago Nascimento (thiago.nascimento@eawag.ch)\n",
    "\n",
    "This notebook is part of the EStreams publication and was used to the computation of the miscelaneous of streamflow indexes provided in this publication.\n",
    "\n",
    "* Note that this code enables not only the replicability of the current database but also the extrapolation to new catchment areas. \n",
    "* Additionally, the user should download and insert the original raw-data in the folder of the same name prior to run this code. \n",
    "* The original third-party data used were not made available in this repository due to redistribution and storage-space reasons.  \n",
    "\n",
    "## Requirements\n",
    "**Python:**\n",
    "\n",
    "* Python>=3.6\n",
    "* Jupyter\n",
    "* geopandas=0.10.2\n",
    "* numpy\n",
    "* os\n",
    "* pandas\n",
    "* tqdm\n",
    "\n",
    "Check the Github repository for an environment.yml (for conda environments) or requirements.txt (pip) file.\n",
    "\n",
    "**Files:**\n",
    "\n",
    "* data/streamflow/estreams_gauging_stations.csv\n",
    "* data/streamflow/estreams_timeseries_runoff.csv\n",
    "* data/shapefiles/estreams_catchments.shp\n",
    "\n",
    "**Directory:**\n",
    "\n",
    "* Clone the GitHub directory locally\n",
    "* Place any third-data variables in their respective directory.\n",
    "* ONLY update the \"PATH\" variable in the section \"Configurations\", with their relative path to the EStreams directory. \n",
    "\n",
    "## References\n",
    "* Do, H. X., Gudmundsson, L., Leonard, M. & Westra, S. The Global Streamflow Indices and Metadata Archive (GSIM)-Part 1: The production of a daily streamflow archive and metadata. Earth Syst Sci Data 10, 765–785 (2018).\n",
    "\n",
    "* Gudmundsson, L., Do, H. X., Leonard, M. & Westra, S. The Global Streamflow Indices and Metadata Archive (GSIM)-Part 2: Quality control, time-series indices and homogeneity assessment. Earth Syst Sci Data 10, 787–804 (2018).\n",
    "\n",
    "## Observations\n",
    "* Here we compute the streamflow indices well-discussed and computed in the GSIM papers (Gudmundsson et al., 2018; Do et al., 2018). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f7648ab",
   "metadata": {},
   "source": [
    "# Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f6e053",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tqdm as tqdm\n",
    "import geopandas as gpd\n",
    "import os\n",
    "from utils.streamflowindices import *\n",
    "from utils.general import calculate_areas_when_0, calculate_specific_discharge\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "134a0050",
   "metadata": {},
   "source": [
    "# Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec3eb41f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only editable variables:\n",
    "# Relative path to your local directory\n",
    "PATH = \"../../..\"\n",
    "# Set some constraints for minimum number of daily measurments at each time-resolution:\n",
    "THRESHOLD_YR = 360 \n",
    "THRESHOLD_MO = 25\n",
    "THRESHOLD_WE = 7\n",
    "THRESHOLD_SE = 80"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b0c1c1e",
   "metadata": {},
   "source": [
    "* #### The users should NOT change anything in the code below here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d25b417",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Non-editable variables:\n",
    "PATH_YR = \"results/timeseries/streamflowindices/yearly/\"\n",
    "PATH_MO = \"results/timeseries/streamflowindices/monthly/\"\n",
    "PATH_WE = \"results/timeseries/streamflowindices/weekly/\"\n",
    "PATH_SE = \"results/timeseries/streamflowindices/seasonally/\"\n",
    "\n",
    "# Set the directory:\n",
    "os.chdir(PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b54907f",
   "metadata": {},
   "source": [
    "# Import data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea546d0",
   "metadata": {},
   "source": [
    "## Daily specific discharge time series\n",
    "It is important to note that this time series was already filtered under a quality-check to delete negative values. The data is in milimeters per day (mm/day).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb14bc3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "timeseries_EU = pd.read_csv(\"data/streamflow/estreams_timeseries_streamflow.csv\", index_col=0)\n",
    "timeseries_EU.index = pd.to_datetime(timeseries_EU.index)\n",
    "timeseries_EU.index.name = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a3c3ffc",
   "metadata": {},
   "source": [
    "## Streamflow gauges network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da333773",
   "metadata": {},
   "outputs": [],
   "source": [
    "network_estreams = pd.read_csv('data/streamflow/estreams_gauging_stations.csv', encoding='utf-8')\n",
    "network_estreams.set_index(\"basin_id\", inplace = True)\n",
    "network_estreams\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71c07b27",
   "metadata": {},
   "source": [
    "## Catchment boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f8e1da",
   "metadata": {},
   "outputs": [],
   "source": [
    "catchment_boundaries = gpd.read_file('data/shapefiles/estreams_catchments.shp')\n",
    "catchment_boundaries.set_index(\"basin_id\", inplace = True)\n",
    "catchment_boundaries.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b17eb94",
   "metadata": {},
   "source": [
    "# Computation processing\n",
    "## Pre-processing:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d611e6f",
   "metadata": {},
   "source": [
    "### Specific discharge computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "727353f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust areas that might be zero: \n",
    "network_estreams = calculate_areas_when_0(network_estreams, catchment_boundaries)\n",
    "network_estreams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b19c1f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specific discharge computation\n",
    "timeseries_EU = calculate_specific_discharge(network_estreams, timeseries_EU)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b33f950",
   "metadata": {},
   "source": [
    "## Other preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f9572dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to compute a mask with the data quality: 1 refers to NaNs and 0 to non-NaNs\n",
    "timeseries_EU_quality = (pd.isna(timeseries_EU)).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee93961e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we use the calendar year, so the hydro_year starts in 1 January\n",
    "hydro_year = np.array(timeseries_EU_quality.index.year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93652270",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply a 7-day backward-looking moving average\n",
    "timeseries_EU_smoothed = timeseries_EU.rolling(window=7, min_periods=1).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f34e2b9d",
   "metadata": {},
   "source": [
    "## Yearly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "967c7bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here all the operations are organized in the form of a \"pipeline\" for organization and progress visuatilization:\n",
    "\n",
    "# List of operations with names, functions, and simplified names for file export:\n",
    "operations = [\n",
    "    ('Mean', lambda x: np.mean(x) if x.count() >= THRESHOLD_YR else np.nan, 'mean'),\n",
    "    ('Standard Deviation', lambda x: np.std(x) if x.count() >= THRESHOLD_YR else np.nan, 'std'),\n",
    "    ('Coefficient of Variation', lambda x: np.var(x) if x.count() >= THRESHOLD_YR else np.nan, 'cv'),\n",
    "    ('Minimum', lambda x: np.min(x) if x.count() >= THRESHOLD_YR else np.nan, 'min'),\n",
    "    ('Maximum', lambda x: np.max(x) if x.count() >= THRESHOLD_YR else np.nan, 'max'),\n",
    "    ('IQR', lambda x: calculate_iqr(x, THRESHOLD_YR), 'iqr'),\n",
    "    ('Percentile 10', lambda x: np.percentile(x, 10) if x.count() >= THRESHOLD_YR else np.nan, 'p10'),\n",
    "    ('Percentile 20', lambda x: np.percentile(x, 20) if x.count() >= THRESHOLD_YR else np.nan, 'p20'),\n",
    "    ('Percentile 30', lambda x: np.percentile(x, 30) if x.count() >= THRESHOLD_YR else np.nan, 'p30'),\n",
    "    ('Percentile 40', lambda x: np.percentile(x, 40) if x.count() >= THRESHOLD_YR else np.nan, 'p40'),\n",
    "    ('Percentile 50', lambda x: np.percentile(x, 50) if x.count() >= THRESHOLD_YR else np.nan, 'p50'),\n",
    "    ('Percentile 60', lambda x: np.percentile(x, 60) if x.count() >= THRESHOLD_YR else np.nan, 'p60'),\n",
    "    ('Percentile 70', lambda x: np.percentile(x, 70) if x.count() >= THRESHOLD_YR else np.nan, 'p70'),\n",
    "    ('Percentile 80', lambda x: np.percentile(x, 80) if x.count() >= THRESHOLD_YR else np.nan, 'p80'),\n",
    "    ('Percentile 90', lambda x: np.percentile(x, 90) if x.count() >= THRESHOLD_YR else np.nan, 'p90'),\n",
    "    ('Minimum 7-days', lambda x: np.min(x) if x.count() >= THRESHOLD_YR else np.nan, 'min7days'),\n",
    "    ('Maximum 7-days', lambda x: np.max(x) if x.count() >= THRESHOLD_YR else np.nan, 'max7days'),\n",
    "    ('Center Timing', None, 'ct'),\n",
    "    ('DOY minimum', None, 'doymin'), \n",
    "    ('DOY maximum', None, 'doymax'),\n",
    "    ('DOY minimum 7-days', None, 'doy7min'), \n",
    "    ('DOY maximum 7-days', None, 'doy7max'), \n",
    "    ('Gini coefficient', None, 'gini')\n",
    "]\n",
    "\n",
    "# Dictionary to store the results of each operation\n",
    "results = {}\n",
    "\n",
    "# Iterate over operations to calculate metrics and export to CSV\n",
    "for op_name, op_func, op_file in tqdm.tqdm(operations, desc='Calculating Metrics'):\n",
    "    \n",
    "    if op_name == 'Minimum 7-days':\n",
    "        \n",
    "        # Calculate the metric using the resample method\n",
    "        results[op_name] = timeseries_EU_smoothed.resample('Y').agg(op_func)\n",
    "    \n",
    "    elif op_name == 'Maximum 7-days':\n",
    "        \n",
    "        # Calculate the metric using the resample method\n",
    "        results[op_name] = timeseries_EU_smoothed.resample('Y').agg(op_func)\n",
    "    \n",
    "    elif op_name == 'Center Timing':\n",
    "        \n",
    "        # Initialize an empty DataFrame to store count threshold results\n",
    "        center_timing_result = pd.DataFrame(index = timeseries_EU.index.year.unique(), \n",
    "                                            columns = timeseries_EU.columns)\n",
    "\n",
    "        # Iterate over gauges and calculate count thresholds\n",
    "        for gauge in timeseries_EU.columns:\n",
    "            center_timing_result[gauge] = calculate_ct(\n",
    "                streamflow=timeseries_EU.loc[:, gauge].values,\n",
    "                quality=timeseries_EU_quality.loc[:, gauge].values,\n",
    "                hydro_year=hydro_year\n",
    "            )\n",
    "        \n",
    "        results[op_name] = center_timing_result\n",
    "        \n",
    "    elif op_name == 'DOY minimum':\n",
    "        \n",
    "        # Initialize an empty DataFrame to store count threshold results\n",
    "        doy_min_result = pd.DataFrame(index = timeseries_EU.index.year.unique(), \n",
    "                                      columns = timeseries_EU.columns)\n",
    "\n",
    "        # Iterate over gauges and calculate count thresholds\n",
    "        for gauge in timeseries_EU.columns:\n",
    "            doy_min_result[gauge] = calculate_min_streamflow_day(\n",
    "                streamflow=timeseries_EU.loc[:, gauge].values,\n",
    "                quality=timeseries_EU_quality.loc[:, gauge].values,\n",
    "                hydro_year=hydro_year\n",
    "            )\n",
    "        \n",
    "        results[op_name] = doy_min_result\n",
    "        \n",
    "    elif op_name == 'DOY maximum':\n",
    "        \n",
    "        # Initialize an empty DataFrame to store count threshold results\n",
    "        doy_max_result = pd.DataFrame(index = timeseries_EU.index.year.unique(), \n",
    "                                      columns = timeseries_EU.columns)\n",
    "\n",
    "        # Iterate over gauges and calculate count thresholds\n",
    "        for gauge in timeseries_EU.columns:\n",
    "            doy_max_result[gauge] = calculate_max_streamflow_day(\n",
    "                streamflow=timeseries_EU.loc[:, gauge].values,\n",
    "                quality=timeseries_EU_quality.loc[:, gauge].values,\n",
    "                hydro_year=hydro_year\n",
    "            )\n",
    "        \n",
    "        results[op_name] = doy_max_result\n",
    "        \n",
    "    elif op_name == 'DOY minimum 7-days':\n",
    "        \n",
    "        # Initialize an empty DataFrame to store count threshold results\n",
    "        doy_7min_result = pd.DataFrame(index = timeseries_EU_smoothed.index.year.unique(), \n",
    "                                      columns = timeseries_EU_smoothed.columns)\n",
    "\n",
    "        # Iterate over gauges and calculate count thresholds\n",
    "        for gauge in timeseries_EU_smoothed.columns:\n",
    "            doy_7min_result[gauge] = calculate_min_streamflow_day(\n",
    "                streamflow=timeseries_EU_smoothed.loc[:, gauge].values,\n",
    "                quality=timeseries_EU_quality.loc[:, gauge].values,\n",
    "                hydro_year=hydro_year\n",
    "            )\n",
    "        \n",
    "        results[op_name] = doy_7min_result\n",
    "        \n",
    "    elif op_name == 'DOY maximum 7-days':\n",
    "        \n",
    "        # Initialize an empty DataFrame to store count threshold results\n",
    "        doy_7max_result = pd.DataFrame(index = timeseries_EU_smoothed.index.year.unique(), \n",
    "                                      columns = timeseries_EU_smoothed.columns)\n",
    "\n",
    "        # Iterate over gauges and calculate count thresholds\n",
    "        for gauge in timeseries_EU_smoothed.columns:\n",
    "            doy_7max_result[gauge] = calculate_max_streamflow_day(\n",
    "                streamflow=timeseries_EU_smoothed.loc[:, gauge].values,\n",
    "                quality=timeseries_EU_quality.loc[:, gauge].values,\n",
    "                hydro_year=hydro_year\n",
    "            )\n",
    "        \n",
    "        results[op_name] = doy_7max_result\n",
    "    \n",
    "    elif op_name == 'Gini coefficient':\n",
    "        \n",
    "        # Initialize an empty DataFrame to store count threshold results\n",
    "        gini_result = pd.DataFrame(index = timeseries_EU.index.year.unique(), \n",
    "                                   columns = timeseries_EU.columns)\n",
    "\n",
    "        # Iterate over gauges and calculate count thresholds\n",
    "        for gauge in timeseries_EU.columns:\n",
    "            gini_result[gauge] = calculate_gini_coefficient(\n",
    "                streamflow=timeseries_EU.loc[:, gauge].values,\n",
    "                quality=timeseries_EU_quality.loc[:, gauge].values,\n",
    "                hydro_year=hydro_year\n",
    "            )\n",
    "        \n",
    "        results[op_name] = gini_result\n",
    "        \n",
    "    else:\n",
    "        # Calculate the metric using the resample method\n",
    "        results[op_name] = timeseries_EU.resample('Y').agg(op_func)\n",
    "        \n",
    "    \n",
    "    # Access results as needed\n",
    "    current_result = results[op_name]\n",
    "\n",
    "    # Export to CSV-file\n",
    "    file_name = f\"yearly_streamflow_{op_file}.csv\"\n",
    "    file_path = os.path.join(PATH_YR, file_name)\n",
    "    current_result = current_result.sort_index(axis=1) # Here we sort the columns\n",
    "    current_result.to_csv(file_path)\n",
    "\n",
    "# Access results as needed\n",
    "streamflow_yr_ave = results['Mean']\n",
    "streamflow_yr_std = results['Standard Deviation']\n",
    "streamflow_yr_cv = results['Coefficient of Variation']\n",
    "streamflow_yr_min = results['Minimum']\n",
    "streamflow_yr_max = results['Maximum']\n",
    "streamflow_yr_iqr = results['IQR']\n",
    "streamflow_yr_p10 = results['Percentile 10']\n",
    "streamflow_yr_p20 = results['Percentile 20']\n",
    "streamflow_yr_p30 = results['Percentile 30']\n",
    "streamflow_yr_p40 = results['Percentile 40']\n",
    "streamflow_yr_p50 = results['Percentile 50']\n",
    "streamflow_yr_p60 = results['Percentile 60']\n",
    "streamflow_yr_p70 = results['Percentile 70']\n",
    "streamflow_yr_p80 = results['Percentile 80']\n",
    "streamflow_yr_p90 = results['Percentile 90']\n",
    "streamflow_yr_ct = results['Center Timing']\n",
    "streamflow_yr_doymin = results['DOY minimum']\n",
    "streamflow_yr_doymax = results['DOY maximum']\n",
    "streamflow_yr_doy7min = results['DOY minimum 7-days']\n",
    "streamflow_yr_doy7max = results['DOY maximum 7-days']\n",
    "streamflow_yr_gini = results['Gini coefficient']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d25086a1",
   "metadata": {},
   "source": [
    "## Monthly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed0e11bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here all the operations are organized in the form of a \"pipeline\" for organization and progress visuatilization:\n",
    "\n",
    "# List of operations with names, functions, and simplified names for file export:\n",
    "operations = [\n",
    "    ('Mean', lambda x: np.mean(x) if x.count() >= THRESHOLD_MO else np.nan, 'mean'),\n",
    "    ('Standard Deviation', lambda x: np.std(x) if x.count() >= THRESHOLD_MO else np.nan, 'std'),\n",
    "    ('Coefficient of Variation', lambda x: np.var(x) if x.count() >= THRESHOLD_MO else np.nan, 'cv'),\n",
    "    ('Minimum', lambda x: np.min(x) if x.count() >= THRESHOLD_MO else np.nan, 'min'),\n",
    "    ('Maximum', lambda x: np.max(x) if x.count() >= THRESHOLD_MO else np.nan, 'max'),\n",
    "    ('Minimum 7-days', lambda x: np.min(x) if x.count() >= THRESHOLD_MO else np.nan, 'min7days'),\n",
    "    ('Maximum 7-days', lambda x: np.max(x) if x.count() >= THRESHOLD_MO else np.nan, 'max7days'),\n",
    "    ('IQR', lambda x: calculate_iqr(x, THRESHOLD_MO), 'iqr')\n",
    "]\n",
    "\n",
    "# Dictionary to store the results of each operation\n",
    "results = {}\n",
    "\n",
    "# Iterate over operations to calculate metrics and export to CSV\n",
    "for op_name, op_func, op_file in tqdm.tqdm(operations, desc='Calculating Metrics'):\n",
    "    \n",
    "    if op_name == 'Minimum 7-days':\n",
    "        \n",
    "        # Calculate the metric using the resample method\n",
    "        results[op_name] = timeseries_EU_smoothed.resample('M').agg(op_func)\n",
    "    \n",
    "    elif op_name == 'Maximum 7-days':\n",
    "        \n",
    "        # Calculate the metric using the resample method\n",
    "        results[op_name] = timeseries_EU_smoothed.resample('M').agg(op_func)  \n",
    "    \n",
    "    else:\n",
    "            \n",
    "        # Calculate the metric using the resample method\n",
    "        results[op_name] = timeseries_EU.resample('M').agg(op_func)\n",
    "    \n",
    "    # Access results as needed\n",
    "    current_result = results[op_name]\n",
    "\n",
    "    # Export to CSV-file\n",
    "    file_name = f\"monthly_streamflow_{op_file}.csv\"\n",
    "    file_path = os.path.join(PATH_MO, file_name)\n",
    "    current_result = current_result.sort_index(axis=1) # Here we sort the columns\n",
    "    current_result.to_csv(file_path)\n",
    "    \n",
    "# Access results as needed\n",
    "streamflow_mo_ave = results['Mean']\n",
    "streamflow_mo_std = results['Standard Deviation']\n",
    "streamflow_mo_cv = results['Coefficient of Variation']\n",
    "streamflow_mo_min = results['Minimum']\n",
    "streamflow_mo_max = results['Maximum']\n",
    "streamflow_mo_min7days = results['Minimum 7-days']\n",
    "streamflow_mo_max7days = results['Maximum 7-days']\n",
    "streamflow_mo_iqr = results['IQR']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9afe884e",
   "metadata": {},
   "source": [
    "## Seasonal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "720a8dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here all the operations are organized in the form of a \"pipeline\" for organization and progress visuatilization:\n",
    "\n",
    "# List of operations with names, functions, and simplified names for file export:\n",
    "operations = [\n",
    "    ('Mean', lambda x: np.mean(x) if x.count() >= THRESHOLD_SE else np.nan, 'mean'),\n",
    "    ('Standard Deviation', lambda x: np.std(x) if x.count() >= THRESHOLD_SE else np.nan, 'std'),\n",
    "    ('Coefficient of Variation', lambda x: np.var(x) if x.count() >= THRESHOLD_SE else np.nan, 'cv'),\n",
    "    ('Minimum', lambda x: np.min(x) if x.count() >= THRESHOLD_SE else np.nan, 'min'),\n",
    "    ('Maximum', lambda x: np.max(x) if x.count() >= THRESHOLD_SE else np.nan, 'max'),\n",
    "    ('IQR', lambda x: calculate_iqr(x, THRESHOLD_SE), 'iqr'),\n",
    "    ('Percentile 10', lambda x: np.percentile(x, 10) if x.count() >= THRESHOLD_SE else np.nan, 'p10'),\n",
    "    ('Percentile 20', lambda x: np.percentile(x, 20) if x.count() >= THRESHOLD_SE else np.nan, 'p20'),\n",
    "    ('Percentile 30', lambda x: np.percentile(x, 30) if x.count() >= THRESHOLD_SE else np.nan, 'p30'),\n",
    "    ('Percentile 40', lambda x: np.percentile(x, 40) if x.count() >= THRESHOLD_SE else np.nan, 'p40'),\n",
    "    ('Percentile 50', lambda x: np.percentile(x, 50) if x.count() >= THRESHOLD_SE else np.nan, 'p50'),\n",
    "    ('Percentile 60', lambda x: np.percentile(x, 60) if x.count() >= THRESHOLD_SE else np.nan, 'p60'),\n",
    "    ('Percentile 70', lambda x: np.percentile(x, 70) if x.count() >= THRESHOLD_SE else np.nan, 'p70'),\n",
    "    ('Percentile 80', lambda x: np.percentile(x, 80) if x.count() >= THRESHOLD_SE else np.nan, 'p80'),\n",
    "    ('Percentile 90', lambda x: np.percentile(x, 90) if x.count() >= THRESHOLD_SE else np.nan, 'p90'),\n",
    "    ('Minimum 7-days', lambda x: np.min(x) if x.count() >= THRESHOLD_SE else np.nan, 'min7days'),\n",
    "    ('Maximum 7-days', lambda x: np.max(x) if x.count() >= THRESHOLD_SE else np.nan, 'max7days')]\n",
    "\n",
    "# Dictionary to store the results of each operation\n",
    "results = {}\n",
    "\n",
    "# Iterate over operations to calculate metrics and export to CSV\n",
    "for op_name, op_func, op_file in tqdm.tqdm(operations, desc='Calculating Metrics'):\n",
    "    \n",
    "    if op_name == 'Minimum 7-days':\n",
    "        \n",
    "        # Calculate the metric using the resample method\n",
    "        results[op_name] = timeseries_EU_smoothed.resample('QS-MAR').agg(op_func)\n",
    "    \n",
    "    elif op_name == 'Maximum 7-days':\n",
    "        \n",
    "        # Calculate the metric using the resample method\n",
    "        results[op_name] = timeseries_EU_smoothed.resample('QS-MAR').agg(op_func)\n",
    "        \n",
    "    else:\n",
    "        # Calculate the metric using the resample method\n",
    "        results[op_name] = timeseries_EU.resample('QS-MAR').agg(op_func)\n",
    "        \n",
    "    \n",
    "    # Access results as needed\n",
    "    current_result = results[op_name]\n",
    "    \n",
    "    # Delete the first row (1899)\n",
    "    current_result = current_result.iloc[1:, :]\n",
    "    \n",
    "    # Export to CSV-file\n",
    "    file_name = f\"seasonally_streamflow_{op_file}.csv\"\n",
    "    file_path = os.path.join(PATH_SE, file_name)\n",
    "    current_result = current_result.sort_index(axis=1) # Here we sort the columns\n",
    "    current_result.to_csv(file_path)\n",
    "\n",
    "# Access results as needed\n",
    "streamflow_se_ave = results['Mean']\n",
    "streamflow_se_std = results['Standard Deviation']\n",
    "streamflow_se_cv = results['Coefficient of Variation']\n",
    "streamflow_se_min = results['Minimum']\n",
    "streamflow_se_max = results['Maximum']\n",
    "streamflow_se_iqr = results['IQR']\n",
    "streamflow_se_p10 = results['Percentile 10']\n",
    "streamflow_se_p20 = results['Percentile 20']\n",
    "streamflow_se_p30 = results['Percentile 30']\n",
    "streamflow_se_p40 = results['Percentile 40']\n",
    "streamflow_se_p50 = results['Percentile 50']\n",
    "streamflow_se_p60 = results['Percentile 60']\n",
    "streamflow_se_p70 = results['Percentile 70']\n",
    "streamflow_se_p80 = results['Percentile 80']\n",
    "streamflow_se_p90 = results['Percentile 90']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09f7d47f",
   "metadata": {},
   "source": [
    "## Weekly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d93c2d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here all the operations are organized in the form of a \"pipeline\" for organization and progress visuatilization:\n",
    "\n",
    "# List of operations with names, functions, and simplified names for file export:\n",
    "operations = [\n",
    "    ('Mean', lambda x: np.mean(x) if x.count() >= THRESHOLD_WE else np.nan, 'mean'),\n",
    "    ('Standard Deviation', lambda x: np.std(x) if x.count() >= THRESHOLD_WE else np.nan, 'std'),\n",
    "    ('Coefficient of Variation', lambda x: np.var(x) if x.count() >= THRESHOLD_WE else np.nan, 'cv'),\n",
    "    ('Minimum', lambda x: np.min(x) if x.count() >= THRESHOLD_WE else np.nan, 'min'),\n",
    "    ('Maximum', lambda x: np.max(x) if x.count() >= THRESHOLD_WE else np.nan, 'max'),\n",
    "    ('IQR', lambda x: calculate_iqr(x, THRESHOLD_WE), 'iqr')\n",
    "]\n",
    "\n",
    "# Dictionary to store the results of each operation\n",
    "results = {}\n",
    "\n",
    "# Iterate over operations to calculate metrics and export to CSV\n",
    "for op_name, op_func, op_file in tqdm.tqdm(operations, desc='Calculating Metrics'):\n",
    "\n",
    "    # Calculate the metric using the resample method\n",
    "    results[op_name] = timeseries_EU.resample('W').agg(op_func)\n",
    "    \n",
    "    # Access results as needed\n",
    "    current_result = results[op_name]\n",
    "\n",
    "    # Export to CSV-file\n",
    "    file_name = f\"weekly_streamflow_{op_file}.csv\"\n",
    "    file_path = os.path.join(PATH_WE, file_name)\n",
    "    current_result = current_result.sort_index(axis=1) # Here we sort the columns\n",
    "    current_result.to_csv(file_path)\n",
    "\n",
    "# Access results as needed\n",
    "streamflow_we_ave = results['Mean']\n",
    "streamflow_we_std = results['Standard Deviation']\n",
    "streamflow_we_cv = results['Coefficient of Variation']\n",
    "streamflow_we_min = results['Minimum']\n",
    "streamflow_we_max = results['Maximum']\n",
    "streamflow_we_iqr = results['IQR']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b608e26e",
   "metadata": {},
   "source": [
    "# End"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
