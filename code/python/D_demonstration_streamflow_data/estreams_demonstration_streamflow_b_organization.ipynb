{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46e94d6f",
   "metadata": {},
   "source": [
    "# Streamflow Catalogue Demonstration: Records organization\n",
    "\n",
    "Author: Thiago Nascimento (thiago.nascimento@eawag.ch)\n",
    "\n",
    "This notebook is part of the EStreams publication and was used to organize most of the downloaded/received streamflow records in a adequate way prior to the data processing. \n",
    "\n",
    "* Note that this code enables not only the replicability of the current database but also the extrapolation to new catchment areas. \n",
    "* Additionally, the user should download and insert the original raw-data in the folder of the same name prior to run this code. \n",
    "* The original third-party data used were not made available in this repository due to redistribution and storage-space reasons.  \n",
    "\n",
    "## Requirements\n",
    "**Python:**\n",
    "\n",
    "* Python>=3.6\n",
    "* Jupyter\n",
    "* numpy\n",
    "* os\n",
    "* pandas\n",
    "* tqdm\n",
    "* tabula\n",
    "\n",
    "Check the Github repository for an environment.yml (for conda environments) or requirements.txt (pip) file.\n",
    "\n",
    "**Files:**\n",
    "\n",
    "* \n",
    "\n",
    "**Directory:**\n",
    "\n",
    "* Clone the GitHub directory locally\n",
    "* Place any third-data variables in their respective directory.\n",
    "* ONLY update the \"PATH\" variable in the section \"Configurations\", with their relative path to the EStreams directory. \n",
    "\n",
    "## References\n",
    "- Please check the Streamflow Catalogue for full information about how to reference the streamflow records. \n",
    "\n",
    "## Observations\n",
    "- This notebook encompasses 27 (out of 38) countries/regions covered in EStreams. The reason is that for some providers, the data was processed in a non-elegant way. Users can expect updates, or can simply adapt the current codes for their needs.\n",
    "- Hence, users should use this notebook as a guidance when processing their own data, and are therefore invited to adapt, adjust and create new scripts based on the current available codes. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f7648ab",
   "metadata": {},
   "source": [
    "# Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bdaf084",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tqdm as tqdm\n",
    "import glob\n",
    "import os\n",
    "import tabula"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "134a0050",
   "metadata": {},
   "source": [
    "# Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec3eb41f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only editable variables:\n",
    "# Relative path to your local directory\n",
    "PATH = \"../../..\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b0c1c1e",
   "metadata": {},
   "source": [
    "* #### The users should NOT change anything in the code below here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d25b417",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the directory:\n",
    "os.chdir(PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b54907f",
   "metadata": {},
   "source": [
    "# Import and process the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea546d0",
   "metadata": {},
   "source": [
    "### 1. Austria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1515cb13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, we specify the path to the folder containing our CSV files:\n",
    "path = r'data/streamflow/raw_data/AT/Q-Tagesmittel'\n",
    "\n",
    "# Next, we list all the filenames in the folder:\n",
    "filenames = glob.glob(path + \"/*.csv\")\n",
    "\n",
    "# Create an empty DataFrame to store the final time series data:\n",
    "timeseries_AT = pd.DataFrame(index=pd.date_range('01-01-1900', '12-31-2022', freq='D'))\n",
    "\n",
    "# Iterate through each file and process its contents:\n",
    "for filename in tqdm.tqdm(filenames):\n",
    "    # Read the CSV file to determine at which row the dataset starts:\n",
    "    aux_1 = pd.read_csv(filename, encoding='latin-1', engine='python', skiprows=0, delimiter=\";\", skip_blank_lines=False)\n",
    "    row_start = int(aux_1.index[aux_1.iloc[:, 0] == \"Werte:\"].tolist()[0])\n",
    "\n",
    "    # Read the CSV file, skipping unnecessary rows:\n",
    "    data = pd.read_csv(filename, encoding='latin-1', engine='python', skiprows=row_start, delimiter=\";\", decimal=\",\")\n",
    "\n",
    "    # Reset index and set dates as index:\n",
    "    data.reset_index(inplace=True)\n",
    "    data[\"dates\"] = pd.to_datetime(data[\"Werte:\"], format='%d.%m.%Y %H:%M:%S')\n",
    "    data.set_index(\"dates\", inplace=True)\n",
    "    data.drop(\"Werte:\", axis=1, inplace=True)\n",
    "    data.columns = [\"Q_m3_s\"]\n",
    "\n",
    "    # Convert non-numeric values to NaN:\n",
    "    data[\"Q_m3_s\"] = data[\"Q_m3_s\"].replace(\" LÃ¼cke\", np.nan)\n",
    "\n",
    "    # Remove whitespace from column contents:\n",
    "    data['Q_m3_s'] = data['Q_m3_s'].str.strip()\n",
    "\n",
    "    # Convert data to numeric:\n",
    "    data[\"Q_m3_s\"] = data[\"Q_m3_s\"].apply(float)\n",
    "    \n",
    "    # Retrieve the station name from the filename:\n",
    "    namestation = os.path.basename(filename)\n",
    "    namestation = namestation.split(\"-\", 2)[2]\n",
    "    namestation = namestation.replace(\".csv\", \"\")\n",
    "    \n",
    "    # Store the data in the final DataFrame, using the station name as a column label:\n",
    "    timeseries_AT.loc[:, int(namestation)] = data.Q_m3_s\n",
    "\n",
    "# Check the final dataset out\n",
    "timeseries_AT.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16c523e1",
   "metadata": {},
   "source": [
    "### 2. Bosnia and H.\n",
    "- Here we actually convert data from yearly hydrological reports in PDF to CSV.\n",
    "- We only used the yearbooks from 1987-2019, since before then, the PDF-files were not possible to be converted to CSV-files.\n",
    "- Observe that there are several steps that are manually adjusted (checked) to make this current code work, such as retrieving the pages with valid information.\n",
    "- For the steps manually adjusted, we make sure to leave the label (manually checked) in order to guide potential users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e8ca28",
   "metadata": {},
   "outputs": [],
   "source": [
    "## BA records (PDF first):\n",
    "path =r'data/streamflow/raw_data/BA'\n",
    "filenames = glob.glob(path + \"/*.pdf\")\n",
    "\n",
    "### Convertion from PDF to CSV:\n",
    "# Here we specify the pages where there is data in each PDF file (manual inspection):\n",
    "# The pdfs should be in chronological order (1987-2019):\n",
    "pages_pdfs = [\"56-83\", \"55-83\", \"56-84\", \"56-84\", \"53-78\", \"47-63\", \"51-67\", \"55-73\", \"91-125\",\n",
    "              \"91-129\", \"101-146\", \"109-152\", \"117-158\", \"120-158\", \"139-189\", \"145-198\", \"151-209\", \"151-213\", \n",
    "              \"162-222\", \"162-230\", \"168-242\", \"168-248\", \"170-254\", \"166-250\"]\n",
    "\n",
    "i = 0\n",
    "for filename in tqdm.tqdm(filenames):\n",
    "    \n",
    "    # Specify the output path for the Excel file\n",
    "    excel_path = filename.replace(\".pdf\", \".csv\")\n",
    "\n",
    "    # Convert the PDF to Excel\n",
    "    tabula.convert_into(filename, excel_path, output_format='csv', pages=pages_pdfs[i])\n",
    "    \n",
    "    i = i + 1\n",
    "\n",
    "### Extraction of the time series\n",
    "path =r'data/streamflow/raw_data/BA'\n",
    "filenames = glob.glob(path + \"/*.csv\")\n",
    "\n",
    "# Here we create a full dataframe for the entire time-series:\n",
    "timeseries_BA = pd.DataFrame(index = pd.date_range('01-01-1987','12-31-2019', freq='D'))\n",
    "\n",
    "# Here we choose only the rows related to the days to keep in our time-series dataframes:\n",
    "rows_to_keep =  [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, \n",
    "                 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]\n",
    "rows_to_keep = list(map(str, rows_to_keep))\n",
    "\n",
    "# This is the sep type (manually checked):\n",
    "sep_type = [\";\", \",\", \";\", \",\", \",\", \",\", \",\", \",\", \";\", \";\", \";\", \",\", \";\", \",\", \";\", \";\", \";\", \",\", \";\", \";\", \";\", \";\", \";\", \";\"]\n",
    "\n",
    "i = 0\n",
    "\n",
    "for filename_net in tqdm.tqdm(filenames_net):\n",
    "        \n",
    "    filename_ts = filenames[i]\n",
    "    \n",
    "    # Here we read the time-series:\n",
    "    timeseries = pd.read_csv(filename_ts, skiprows=0, header = 0, decimal=',', sep = sep_type[i])\n",
    "    \n",
    "    # Here we read the stations for the time-series:\n",
    "    stations_df = pd.read_csv(filename_net,  encoding='unicode_escape', sep= \";\")\n",
    "    \n",
    "    # Replace non-properly read characters with '?'\n",
    "    stations_df = stations_df.astype(str)\n",
    "    stations_df = stations_df.apply(lambda x: x.str.encode('unicode_escape', 'replace').str.decode('utf-8'))\n",
    "    stations = stations_df.iloc[:, 1].tolist()\n",
    "    \n",
    "    # Now we select only the rows that refer to days to keep:\n",
    "    timeseries = timeseries[timeseries.iloc[:, 0].isin(rows_to_keep)]\n",
    "    timeseries.reset_index(inplace=True, drop=True)\n",
    "    \n",
    "    # First we ensure that all is an object:\n",
    "    timeseries = timeseries.astype(str)\n",
    "\n",
    "    # Now we convert to float after replacing any comma to point:\n",
    "    timeseries = timeseries.applymap(lambda x: float(x.replace(',', '.')))\n",
    "    \n",
    "    # Here we adjust the column names:\n",
    "    timeseries.columns = [\"Day\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\", \"10\", \"11\", \"12\"]\n",
    "    \n",
    "    # Here we get the year:\n",
    "    year_station = os.path.basename(filename_ts)\n",
    "    year_station = year_station.split(\" \", 2)[0]\n",
    "\n",
    "    # This part is done for each date:\n",
    "    timeseries_numpy = timeseries.iloc[:, :].to_numpy()\n",
    "    timeseries_aux_df = pd.DataFrame(index = range(31*12), columns =stations)\n",
    "    timeseries_aux_df[\"dates\"] = np.nan\n",
    "\n",
    "\n",
    "    # First we do the loop at each month:\n",
    "    rows_dia_aux = 0\n",
    "    for mes in range(1, 13):\n",
    "        row_ts_aux = 0\n",
    "    \n",
    "        # Now we do at each station:\n",
    "        k = 0\n",
    "        for station in stations:\n",
    "        \n",
    "            for dia in range(31):\n",
    "            \n",
    "                # Here we insert the time-series value:\n",
    "                timeseries_aux_df.iloc[rows_dia_aux + dia, k] = timeseries_numpy[row_ts_aux, mes]\n",
    "\n",
    "                # Here we insert the \"date\":\n",
    "                timeseries_aux_df.iloc[rows_dia_aux + dia, -1] = str(dia + 1) + \"-\" + str(mes) + \"-\" + str(year_station)\n",
    "        \n",
    "                # Here we get the rows for getting the data:\n",
    "                row_ts_aux = row_ts_aux + 1\n",
    "        \n",
    "            # Here it is the column:\n",
    "            k = k + 1\n",
    "        \n",
    "\n",
    "        rows_dia_aux = rows_dia_aux + 31    \n",
    "    \n",
    "\n",
    "    # At this part we convert our previously written datetime column to properlly a datetime type. \n",
    "    # However, we have to use the \"try\" to deal with the rows that do not present real dates (e.g., 31.02.2022): \n",
    "\n",
    "    for dia in range(len(timeseries_aux_df)):    \n",
    "        try:\n",
    "            timeseries_aux_df.loc[dia, \"dates\"] = pd.to_datetime(timeseries_aux_df.iloc[dia, -1], format='%d-%m-%Y')\n",
    "    \n",
    "        except:\n",
    "            timeseries_aux_df.loc[dia, \"dates\"] = np.nan\n",
    "        \n",
    "    ## Here we delete the rows that do not present real dates (e.g., 31.02.2022): \n",
    "    timeseries_aux_df = timeseries_aux_df.dropna(subset=['dates'])\n",
    "    timeseries_aux_df.set_index('dates', inplace = True)\n",
    "\n",
    "    # Convert non-numeric values to NaN for the entire DataFrame\n",
    "    timeseries_aux_df = timeseries_aux_df.apply(pd.to_numeric, errors='coerce')\n",
    "    \n",
    "    \n",
    "    # Here we assign the value of the year at the correct location:\n",
    "    timeseries_BA.loc[timeseries_aux_df.index, timeseries_aux_df.columns] = timeseries_aux_df\n",
    "    \n",
    "    i = i + 1\n",
    "\n",
    "# Check the final dataset out\n",
    "timeseries_BA.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fdf999c",
   "metadata": {},
   "source": [
    "### 3. GRDC data \n",
    "- Countries used: BG, BY, CY, EE, GR, HU, IT, LT, LV, MD, MK, RO, RS, RU, SK, TR, UA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f72d8eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## GRDC network:\n",
    "# Read the data\n",
    "network_GRDC = pd.read_excel(r'data/streamflow/raw_data/GRDC/GRDC_Stations.xlsx', skiprows=0)\n",
    "\n",
    "# Filter only Europe (wmo_reg == 6)\n",
    "network_GRDC = network_GRDC[network_GRDC.wmo_reg == 6]\n",
    "\n",
    "## GRDC time series\n",
    "# First we can read our files within the folder:\n",
    "path =r'data/raw_data/GRDC'\n",
    "filenames = glob.glob(path + \"/*.txt\")\n",
    "\n",
    "timeseries_GRDC = pd.DataFrame(index = pd.date_range('01-01-1900','12-31-2022', freq='D'))\n",
    "\n",
    "# Iterate through each file and process its contents:\n",
    "for filename in tqdm.tqdm(filenames):\n",
    "    # First we read our time-series:\n",
    "    data = pd.read_csv(filename, encoding='latin-1', engine='python', skiprows = 36, delimiter = \";\", usecols = [0, 2])\n",
    "\n",
    "    # We define new column names:\n",
    "    names = ['dates', 'Qm3s']\n",
    "    data.columns = names\n",
    "\n",
    "    # Convert our column of dates to datetime format:\n",
    "    data[\"dates\"] = pd.to_datetime(data[\"dates\"], format='%Y-%m-%d')\n",
    "\n",
    "    # We can replace the -999.0 to np.nan:\n",
    "    data.Qm3s.replace(-999.0, np.nan, inplace=True)\n",
    "\n",
    "    # Here we can set the index as the dates column:\n",
    "    data.set_index(\"dates\", inplace = True)\n",
    "\n",
    "    # First we can retrieve the station name:\n",
    "    namestation = os.path.basename(filename)\n",
    "    namestation = namestation.split(\"_\", 1)[0]\n",
    "    \n",
    "    timeseries_GRDC.loc[:, int(namestation)] = data.Qm3s\n",
    "\n",
    "# Check the final time series:\n",
    "timeseries_GRDC.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "561d005b",
   "metadata": {},
   "source": [
    "### 4. Switzerland"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f40e857",
   "metadata": {},
   "outputs": [],
   "source": [
    "## CAMELS-CH network:\n",
    "# Read the data\n",
    "network_CH = pd.read_csv(r'data/streamflow/raw_data/CH/CAMELS_CH_topographic_attributes.csv', skiprows=1, encoding='latin-1', sep= \";\")\n",
    "network_CH = network_CH.set_index(\"gauge_id\")\n",
    "\n",
    "## CAMELS-CH time series\n",
    "# First we can check our files within the folder:\n",
    "path =r'data/streamflow/raw_data/CH'\n",
    "filenames = glob.glob(path + \"/*.csv\")\n",
    "\n",
    "timeseries_CH = pd.DataFrame(index = pd.date_range('01-01-1981','12-31-2022', freq='D'))\n",
    "\n",
    "# Iterate through each file and process its contents:\n",
    "for filename in tqdm.tqdm(filenames):\n",
    "    # First we read our time-series:\n",
    "    data = pd.read_csv(filename, encoding='latin-1', engine='python', skiprows = 0, delimiter = \";\", usecols = [0, 1, 2])\n",
    "\n",
    "    # We define new column names:\n",
    "    names = ['dates', 'Qm3s', \"Qmmday\"]\n",
    "    data.columns = names\n",
    "\n",
    "    # Convert our column of dates to datetime format:\n",
    "    data[\"dates\"] = pd.to_datetime(data[\"dates\"], format='%Y-%m-%d')\n",
    "\n",
    "    # Here we can set the index as the dates column:\n",
    "    data.set_index(\"dates\", inplace = True)\n",
    "\n",
    "    # First we can retrieve the station name:\n",
    "    namestation = os.path.basename(filename)\n",
    "    namestation = namestation.split(\"_\", 5)[4]\n",
    "    namestation = namestation.replace(\".csv\", \"\")\n",
    "    timeseries_CH.loc[:, int(namestation)] = data.Qm3s\n",
    "\n",
    "# Check the final time series:\n",
    "timeseries_CH.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3de279b",
   "metadata": {},
   "source": [
    "### 5. Spain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "171f4b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Spanish time series\n",
    "# First we can check our files within the folder:\n",
    "path =r'data/streamflow/raw_data/ES'\n",
    "filenames = glob.glob(path + \"/*/\")\n",
    "\n",
    "timeseries_ES = pd.DataFrame(index = pd.date_range('01-01-1900','12-31-2022', freq='D'))\n",
    "i = 0\n",
    "for filename in tqdm.tqdm(filenames):\n",
    "    \n",
    "    ## This part is regardig the network information data:\n",
    "    network_filename = pd.read_csv(filename + \"estaf.csv\", encoding='latin-1', skiprows=0, \n",
    "                          usecols = ['indroea', 'lugar', 'suprest', 'suprcnc', 'alti', 'num_cuenca',\n",
    "                                     'long', 'lat', 'xetrs89', 'yetrs89'], \n",
    "                      delimiter = \";\")\n",
    "    \n",
    "    # Now the concatatanion is made to get the full network:\n",
    "    if i == 0:\n",
    "        network = network_filename\n",
    "    else:\n",
    "        network = pd.concat([network, network_filename], axis=0)\n",
    "    i = i + 1\n",
    "    \n",
    "    ## This part is regarding the streamflow data:\n",
    "    timespain = pd.read_csv(filename + \"afliq.csv\", encoding='latin-1', delimiter = \";\")\n",
    "\n",
    "    names = ['ID', 'date', 'height_m', 'Q_m3_s']\n",
    "\n",
    "    timespain.columns = names\n",
    "    timespain[\"date\"] = pd.to_datetime(timespain[\"date\"], format='%d/%m/%Y')\n",
    "    \n",
    "\n",
    "    stationsspain = timespain[\"ID\"].unique().tolist()\n",
    "    \n",
    "    #print(len(stationsspain)) # Just to check the number of stations available at each dataset\n",
    "    \n",
    "    # Here we fill our streamflow datasets into the single dataframe:\n",
    "    for station in stationsspain:\n",
    "\n",
    "        \n",
    "        timespain_station = timespain[timespain.ID == station]\n",
    "        timespain_station.set_index('date', inplace = True)\n",
    "\n",
    "        timeseries_ES.loc[:, station] = timespain_station.Q_m3_s\n",
    "\n",
    "# Check the final time series:\n",
    "timeseries_ES.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9135dc4",
   "metadata": {},
   "source": [
    "### 6. France"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba32bc27",
   "metadata": {},
   "outputs": [],
   "source": [
    "## French time series\n",
    "# First we can check our files within the folder:\n",
    "path =r'data/streamflow/raw_data/FR'\n",
    "filenames = glob.glob(path + \"/*.csv\")\n",
    "\n",
    "timeseries_FR = pd.DataFrame(index = pd.date_range('01-01-1840','12-31-2023', freq='D'))\n",
    "timeseries_quality_FR = pd.DataFrame(index = pd.date_range('01-01-1840','12-31-2023', freq='D'))\n",
    "\n",
    "# Iterate through each file and process its contents:\n",
    "for filename in tqdm.tqdm(filenames):\n",
    "\n",
    "    namestation = os.path.basename(filename)\n",
    "    namestation = namestation.split(\"_\", 1)[0]\n",
    "    \n",
    "    data = pd.read_csv(filename, encoding='latin-1', engine='python', usecols = ['Date (TU)', 'Valeur (en mÃÂ³/s)', \n",
    "                      'Qualification'])\n",
    "\n",
    "    names = ['dates', 'Qm3s', 'Status']\n",
    "    data.columns = names\n",
    "\n",
    "    data[\"dates\"].replace({'T': ' '}, regex=True, inplace=True)\n",
    "    data[\"dates\"].replace({'Z': ' '}, regex=True, inplace=True)\n",
    "\n",
    "    data[\"dates\"] = pd.to_datetime(data['dates'], format='%Y/%m/%d')\n",
    "    data.set_index(\"dates\", inplace = True)\n",
    "    \n",
    "    timeseries_FR.loc[:, str(namestation)] = data.Qm3s\n",
    "    timeseries_quality_FR.loc[:, str(namestation)] = data.Status\n",
    "\n",
    "# Check the final time series:\n",
    "timeseries_FR.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d5a5163",
   "metadata": {},
   "source": [
    "### 7. United kingdom (GB & NI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a71c659",
   "metadata": {},
   "outputs": [],
   "source": [
    "## UK time series\n",
    "# First we can check our files within the folder:\n",
    "path =r'data/streamflow/raw_data/UK'\n",
    "filenames = glob.glob(path + \"/*.csv\")\n",
    "\n",
    "timeseries_UK = pd.DataFrame(index = pd.date_range('01-01-1900','12-31-2022', freq='D'))\n",
    "\n",
    "# Iterate through each file and process its contents:\n",
    "for filename in tqdm.tqdm(filenames):\n",
    "    \n",
    "    # The number of rows before the real streamflow data is variable, then we can solve it as:\n",
    "    # First we select the row where we have the second column with the value \"last\", and we know that the stremflow starts\n",
    "    # right after that:\n",
    "    dataframefull = pd.read_csv(filename, encoding='latin-1', engine='python', skiprows = 0, delimiter = \",\", header = None)\n",
    "    rownnumber = dataframefull.index[dataframefull[1] == \"last\"].tolist()[0]\n",
    "    \n",
    "    # Now we read our time-series:\n",
    "    data = pd.read_csv(filename, encoding='latin-1', engine='python', skiprows = rownnumber + 1, \n",
    "                       delimiter = \",\", usecols = [0, 1], names = [\"dates\", \"Qm3s\"])\n",
    "\n",
    "    # Convert our column of dates to datetime format:\n",
    "    data[\"dates\"] = pd.to_datetime(data[\"dates\"], format='%Y-%m-%d')\n",
    "\n",
    "    # Here we can set the index as the dates column:\n",
    "    data.set_index(\"dates\", inplace = True)\n",
    "\n",
    "    # Here we can retrieve the station name:\n",
    "    namestation = dataframefull.iloc[3, 2]\n",
    "    \n",
    "    timeseries_UK.loc[:, int(namestation)] = data.Qm3s\n",
    "\n",
    "# Check the final time series:\n",
    "timeseries_UK.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b105fc06",
   "metadata": {},
   "source": [
    "### 8. Greece\n",
    "- HCMR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0629250b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Greek time series from HCMR\n",
    "# First we can check our files within the folder:\n",
    "path =r'data/streamflow/raw_data/GR/HCMR'\n",
    "filenames = glob.glob(path + \"/*.csv\")\n",
    "\n",
    "timeseries_GRHCMR = pd.DataFrame(index = pd.date_range('01-01-2014','12-31-2023', freq='D'))\n",
    "\n",
    "# Iterate through each file and process its contents:\n",
    "for filename in tqdm.tqdm(filenames):\n",
    "    namestation = os.path.basename(filename)\n",
    "    namestation = namestation.split(\"-\", 3)[1]\n",
    "\n",
    "    # Here you can get rid of the space before the variable name:\n",
    "    namestation = namestation.lstrip()\n",
    "\n",
    "    # Here we read our data:\n",
    "    timeseries_filename = pd.read_csv(filename, encoding='latin-1', header = 1, delimiter = \",\", \n",
    "                                   decimal = \".\",\n",
    "                                   names = [ \"dates\", \"Q_m3s\"])\n",
    "\n",
    "    # Here we replace the \"cms\" to nothing at the time-series:\n",
    "    timeseries_filename['Q_m3s'] = timeseries_filename['Q_m3s'].str.replace(' cms', '')\n",
    "\n",
    "    # Now we convert our dataset to datetime:\n",
    "    timeseries_filename[\"dates\"] = pd.to_datetime(timeseries_filename[\"dates\"], format='%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "    timeseries_filename.set_index(\"dates\", inplace = True)\n",
    "\n",
    "    # Convert the 'your_column_name' column to numeric\n",
    "    timeseries_filename['Q_m3s'] = pd.to_numeric(timeseries_filename['Q_m3s'], errors='coerce')\n",
    "\n",
    "    # Resample the data to daily intervals, taking the mean of each day\n",
    "    timeseries_filename = timeseries_filename.resample('D').mean()\n",
    "    \n",
    "    timeseries_GRHCMR.loc[:, namestation] = timeseries_filename.Q_m3s\n",
    "\n",
    "# Check the final time series:\n",
    "timeseries_GRHCMR.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "284cc9e7",
   "metadata": {},
   "source": [
    "### 9. Ireland\n",
    "- EPA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d083c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Irish time series from EPA\n",
    "path =r'data/streamflow/raw_data/IE/EPA'\n",
    "filenames = glob.glob(path + \"/*.csv\")\n",
    "\n",
    "timeseries_IEEPA = pd.DataFrame(index = pd.date_range('01-01-1900','12-31-2023', freq='D'))\n",
    "\n",
    "# Iterate through each file and process its contents:\n",
    "for filename in filenames:\n",
    "    # Here we retrieve only the station code:\n",
    "    dataframefull = pd.read_csv(filename,  delimiter = \";\", nrows = 3, header = None)\n",
    "    station = dataframefull.iloc[1, 1]\n",
    "    \n",
    "    # It seems that we download some stage as well, so we have to get rid of those:\n",
    "    if dataframefull.iloc[2, 1] != \"River Discharge\":\n",
    "        pass\n",
    "    \n",
    "    else:\n",
    "        # Now we read our time-series:\n",
    "        dataf = pd.read_csv(filename, skiprows = 7, delimiter = \";\", usecols=range(3), names = [\"dates\", \"Qm3s\", \"quality\"])\n",
    "    \n",
    "        # Convert our column of dates to datetime format:\n",
    "        dataf[\"dates\"] = pd.to_datetime(dataf[\"dates\"], format='%Y-%m-%d')\n",
    "        dataf.set_index(\"dates\", inplace = True)\n",
    "        data = dataf.resample('D').mean()\n",
    "    \n",
    "        data[\"quality\"] = dataf.quality.values\n",
    "    \n",
    "        timeseries_IEEPA.loc[:, str(station)] = data.Qm3s\n",
    "\n",
    "# Check the final time series:\n",
    "timeseries_IEEPA.head()        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfddf32b",
   "metadata": {},
   "source": [
    "- OPW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bac05016",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Irish time series from OPW\n",
    "# Data lecture\n",
    "path =r'data/streamflow/raw_data/IE/OPW'\n",
    "filenames = glob.glob(path + \"/*.txt\")\n",
    "\n",
    "timeseries_IEOPW= pd.DataFrame(index = pd.date_range('01-01-1900','12-31-2023', freq='D'))\n",
    "\n",
    "# Iterate through each file and process its contents:\n",
    "for filename in filenames:\n",
    "    # Here we retrieve only the station code:\n",
    "    dataframefull = pd.read_csv(filename,  delimiter = \"\\t\", nrows = 6, header = None)\n",
    "    station = str(dataframefull.iloc[1, 1])\n",
    "    \n",
    "    # It seems that we download some stage as well, so we have to get rid of those:\n",
    "    if dataframefull.iloc[5, 1] != \"cubic meter per second\":\n",
    "        1 + 1\n",
    "    \n",
    "    else:\n",
    "        # Now we read our time-series:\n",
    "        data = pd.read_csv(filename, skiprows = 8, delimiter = \"\\t\", usecols=range(3), names = [\"dates\", \"Qm3s\", \"quality\"])\n",
    "        \n",
    "        # Convert our column of dates to datetime format:\n",
    "        data[\"dates\"] = pd.to_datetime(data[\"dates\"], format='%Y/%m/%d')\n",
    "        data.set_index(\"dates\", inplace = True)\n",
    "    \n",
    "        timeseries_IEOPW.loc[:, str(station)] = data.Qm3s\n",
    "\n",
    "# Check the final time series:\n",
    "timeseries_IEOPW.head() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d840b40",
   "metadata": {},
   "source": [
    "### 10. Iceland"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00a9bc4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Icelandic time series from OPW\n",
    "# First we can check our files within the folder:\n",
    "path =r'data/streamflow/raw_data/IS'\n",
    "filenames = glob.glob(path + \"/*.csv\")\n",
    "\n",
    "timeseries_IS = pd.DataFrame(index = pd.date_range('01-01-1950','12-31-2023', freq='D'))\n",
    "\n",
    "# Iterate through each file and process its contents:\n",
    "for filename in tqdm.tqdm(filenames):\n",
    "    # First we read our time-series:\n",
    "    data = pd.read_csv(filename, encoding='latin-1', engine='python', delimiter = \";\")\n",
    "\n",
    "    # Convert our column of dates to datetime format:\n",
    "    data[\"date\"] = data[\"DD\"].astype(str) + \"-\" + data[\"MM\"].astype(str)  + \"-\" + data[\"YYYY\"].astype(str) \n",
    "    data[\"date\"] = pd.to_datetime(data[\"date\"], format='%d-%m-%Y')\n",
    "\n",
    "    # Here we can set the index as the dates column:\n",
    "    data.set_index(\"date\", inplace = True)\n",
    "\n",
    "    # First we can retrieve the station name:\n",
    "    namestation = os.path.basename(filename)\n",
    "    namestation = namestation.split(\"_\", 1)[1]\n",
    "    # Remove \".csv\" from the string\n",
    "    namestation = namestation.replace(\".csv\", \"\")\n",
    "    \n",
    "    timeseries_IS.loc[:, int(namestation)] = data.qobs\n",
    "\n",
    "# Check the final time series:\n",
    "timeseries_IS.head() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d473f14d",
   "metadata": {},
   "source": [
    "### 11. Italy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6d3069d",
   "metadata": {},
   "source": [
    "- Emilia-Romagna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1cce55b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Italian time series from Emilia-Romagna\n",
    "# First we can check our files within the folder:\n",
    "path =r'data/streamflow/raw_data/IT/EMI'\n",
    "filenames = glob.glob(path + \"/*.csv\")\n",
    "\n",
    "timeseries_ITEM = pd.DataFrame(index = pd.date_range('01-01-1950','12-31-2022', freq='D'))\n",
    "network_ITEM = pd.DataFrame(np.empty((len(filenames),10)))\n",
    "\n",
    "# Initialize counter\n",
    "i = 0\n",
    "\n",
    "# Loop through each filename using tqdm to track progress\n",
    "for filename in tqdm.tqdm(filenames):\n",
    "    \n",
    "    # Read network file with specific encoding, skiprows, delimiter, and footer\n",
    "    network_filename = pd.read_csv(filename, encoding='latin-1', skiprows=4, delimiter=\",\",  \n",
    "                                   skipfooter=6, names=[\"Date_start\", \"dates\", \"Q_m3s\"])\n",
    "\n",
    "    # Convert 'dates' column to datetime format\n",
    "    network_filename[\"dates\"] = pd.to_datetime(network_filename[\"dates\"].astype(str).str[:10])\n",
    "    \n",
    "    # Set 'dates' column as index\n",
    "    network_filename.set_index(\"dates\", inplace=True)\n",
    "    \n",
    "    # Get the number of lines in the file\n",
    "    num_lines = len(network_filename)\n",
    "    \n",
    "    # Read info file with specific encoding, skiprows, delimiter, and footer\n",
    "    info_filename = pd.read_csv(filename, encoding='latin-1', skiprows=num_lines + 4, delimiter=\",\",  \n",
    "                                skipfooter=2)\n",
    "    \n",
    "    # Extract station information\n",
    "    station = info_filename.iloc[0, 0]\n",
    "    \n",
    "    # Assign station info to the corresponding row in network_ITEM DataFrame\n",
    "    network_ITEM.iloc[i, :] = info_filename.iloc[0, :]\n",
    "    \n",
    "    # Assign discharge data to the corresponding column in timeseries_ITEM DataFrame\n",
    "    timeseries_ITEM.loc[:, station] = network_filename.Q_m3s\n",
    "    \n",
    "    # Increment counter\n",
    "    i += 1\n",
    "    \n",
    "# Set column names of network_ITEM DataFrame based on info_filename\n",
    "network_ITEM.columns = info_filename.columns\n",
    "\n",
    "# Check the final time series:\n",
    "timeseries_ITEM.head() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb265d17",
   "metadata": {},
   "source": [
    "- Umbria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe5ab9aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Italian time series from Umbria\n",
    "# First we can check our files within the folder:\n",
    "path =r'data/streamflow/raw_data/IT/UMB'\n",
    "filename = glob.glob(path + \"/*.xlsx\")\n",
    "len(filename)\n",
    "\n",
    "### Retrieve first the stations list:\n",
    "timeseries = pd.read_excel(filename[0], skiprows=0, \n",
    "                          usecols = ['ID_SENSORE_DETTAGLIO', 'NOME_STAZIONE', 'LATITUDINE', 'LONGITUDINE', 'ANNO', 'MESE',\n",
    "                                     'GIORNO', 'AVGDAY'])\n",
    "\n",
    "timeseries[\"dates\"] = timeseries['ANNO'].astype(str) + \"-\" + timeseries[\"MESE\"].astype(str) + \"-\" + timeseries[\"GIORNO\"].astype(str)\n",
    "timeseries[\"dates\"] = pd.to_datetime(timeseries[\"dates\"], format='%Y-%m-%d')\n",
    "timeseries.drop([\"ANNO\", \"MESE\", \"GIORNO\"], axis = 1, inplace = True)\n",
    "\n",
    "names = ['Code', 'Name', 'Lat', 'Lon', \"Q_m3_s\", \"dates\"]\n",
    "\n",
    "timeseries.columns = names\n",
    "\n",
    "stations_list_ITUM = timeseries[\"Code\"].unique().tolist()\n",
    "\n",
    "### Retrieve and organize the time series:\n",
    "# Create a DataFrame to store the final time series data with dates as index\n",
    "timeseries_ITUM = pd.DataFrame(index=pd.date_range('01-01-1925', '12-31-2022', freq='D'))\n",
    "\n",
    "# Create an empty DataFrame to store network information for Umbria\n",
    "network_ITUM = pd.DataFrame(index=range(12), columns=['Code', 'Name', 'Lat', 'Lon'])\n",
    "\n",
    "# Initialize counter\n",
    "i = 0\n",
    "\n",
    "# Loop through each station in the list, tqdm is used to track progress\n",
    "for station in tqdm.tqdm(stations_list_ITUM):\n",
    "    \n",
    "    # Filter time series data for the current station\n",
    "    timeseries_station = timeseries[timeseries.Code == station]\n",
    "    \n",
    "    # Set 'dates' column as index\n",
    "    timeseries_station.set_index('dates', inplace=True)\n",
    "\n",
    "    # Remove duplicate dates if any\n",
    "    timeseries_station = timeseries_station.loc[timeseries_station.index.drop_duplicates(keep=False), :]\n",
    "    \n",
    "    # Assign discharge data to the corresponding column in timeseriesfinal DataFrame\n",
    "    timeseries_ITUM.loc[:, station] = timeseries_station.Q_m3_s\n",
    "    \n",
    "    # Assign station information to the corresponding row in network_umbria DataFrame\n",
    "    network_ITUM.iloc[i, :] = timeseries_station.iloc[0, [0, 1, 2, 3]]\n",
    "    \n",
    "    # Increment counter\n",
    "    i += 1\n",
    "\n",
    "# Check the final time series:\n",
    "timeseries_ITUM.head() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0db1723e",
   "metadata": {},
   "source": [
    "- Vale do Aosta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf57ea3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Italian time series from Vale D'aosta\n",
    "# First we can check our files within the folder:\n",
    "path =r'data/streamflow/raw_data/IT/VAL'\n",
    "filenames = glob.glob(path + \"/*.csv\")\n",
    "\n",
    "# Create a DataFrame to store the final time series data with dates as index\n",
    "timeseries_ITVA = pd.DataFrame(index=pd.date_range('01-01-1978', '12-31-2022', freq='D'))\n",
    "\n",
    "# Initialize counter\n",
    "i = 0\n",
    "\n",
    "# Loop through each filename in the list of filenames, tqdm is used to track progress\n",
    "for filename in tqdm.tqdm(filenames):\n",
    "    \n",
    "    # Read time series data from the current file\n",
    "    timeseries_station = pd.read_csv(filename, encoding='latin-1', skiprows=7, delimiter=\";\", header=None, \n",
    "                                      names=[\"dates\", \"Q_m3_s\"], decimal=',')\n",
    "    \n",
    "    # Convert 'dates' column to datetime format\n",
    "    timeseries_station[\"dates\"] = pd.to_datetime(timeseries_station[\"dates\"], format='%Y-%m-%d')\n",
    "    \n",
    "    # Set 'dates' column as index\n",
    "    timeseries_station.set_index(\"dates\", inplace=True)\n",
    "    \n",
    "    # Remove duplicate dates if any\n",
    "    timeseries_station = timeseries_station.loc[timeseries_station.index.drop_duplicates(keep=False), :]\n",
    "    \n",
    "    # Read station information from the current file\n",
    "    station = pd.read_csv(filename, encoding='latin-1', skiprows=2, delimiter=\";\", header=None, nrows=1)\n",
    "    station = station.iloc[:, 0].str.replace('Stazione: ',' ')\n",
    "    \n",
    "    # Assign discharge data to the corresponding column in timeseriesfinal DataFrame\n",
    "    timeseries_ITVA.loc[:, station[0]] = timeseries_station.Q_m3_s\n",
    "    \n",
    "    # Increment counter\n",
    "    i += 1\n",
    "\n",
    "# Check the final time series:\n",
    "timeseries_ITVA.head()     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c4484fe",
   "metadata": {},
   "source": [
    "- Piemonte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b7d59ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Italian time series from Piemonte\n",
    "# First we can check our files within the folder:\n",
    "path =r'data/streamflow/raw_data/IT/PIE'\n",
    "filenames = glob.glob(path + \"/*.csv\")\n",
    "\n",
    "# Initialize a DataFrame to store the final time series data with dates as index:\n",
    "timeseries_ITPI = pd.DataFrame(index=pd.date_range('01-01-1990', '12-31-2022', freq='D'))\n",
    "\n",
    "# Initialize a counter variable:\n",
    "i = 0\n",
    "\n",
    "# Iterate through each file and process its contents:\n",
    "for filename in tqdm.tqdm(filenames):\n",
    "    # Extract the station name from the filename:\n",
    "    namestation = os.path.basename(filename)\n",
    "    namestation = namestation.split(\"_\", 1)[0]\n",
    "    \n",
    "    # Read the CSV file containing the time series data for the station:\n",
    "    timeseries_station = pd.read_csv(filename, delimiter=\";\", encoding='latin-1', usecols=[0, 1], decimal=',')\n",
    "    timeseries_station.columns = [\"dates\", \"Q_m3_s\"]\n",
    "    \n",
    "    # Convert the 'dates' column to datetime format and set it as index:\n",
    "    timeseries_station[\"dates\"] = pd.to_datetime(timeseries_station[\"dates\"], format='%d/%m/%Y')\n",
    "    timeseries_station.set_index(\"dates\", inplace=True)\n",
    "    \n",
    "    # Store the time series data for the station in the final DataFrame:\n",
    "    timeseries_ITPI.loc[:, namestation] = timeseries_station.Q_m3_s\n",
    "    \n",
    "    # Increment the counter:\n",
    "    i = i + 1\n",
    "\n",
    "# Check the final time series:\n",
    "timeseries_ITPI.head()   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f29e5cc0",
   "metadata": {},
   "source": [
    "- Trento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c558ba14",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Italian time series from Trento\n",
    "# First we can check our files within the folder:\n",
    "path =r'data/streamflow/raw_data/TRE'\n",
    "filenames = glob.glob(path + \"/*.csv\")\n",
    "\n",
    "# Initialize a DataFrame to store the final time series data with dates as index:\n",
    "timeseries_ITTR = pd.DataFrame(index=pd.date_range('10-01-1978', '12-31-2022', freq='D'))\n",
    "\n",
    "# Iterate through each file and process its contents:\n",
    "for filename in filenames:\n",
    "    # Read the CSV file containing the time series data:\n",
    "    timeseries_station = pd.read_csv(filename, delimiter=\",\", encoding='latin-1', skiprows=6, decimal='.')\n",
    "    timeseries_station.columns = [\"dates\", \"Q_m3_s\"]\n",
    "    \n",
    "    # Convert the 'dates' column to datetime format and set it as index:\n",
    "    timeseries_station[\"dates\"] = pd.to_datetime(timeseries_station[\"dates\"], format='%Y-%m-%d')\n",
    "    timeseries_station.set_index(\"dates\", inplace=True)\n",
    "\n",
    "    # Resample the data to fill any missing dates and compute daily means:\n",
    "    timeseries_station = timeseries_station.resample('D').mean()\n",
    "\n",
    "    # Extract the station name from the file and store it:\n",
    "    namestation = pd.read_csv(filename, delimiter=\",\", encoding='latin-1', skiprows=3, nrows=1)\n",
    "    namestation = namestation.iloc[0, 1]\n",
    "    \n",
    "    # Store the time series data for the station in the final DataFrame:\n",
    "    timeseries_ITTR.loc[:, namestation] = timeseries_station.Q_m3_s\n",
    "    \n",
    "# Check the final time series:\n",
    "timeseries_ITTR.head() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06428a36",
   "metadata": {},
   "source": [
    "- Toscana"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb347ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Italian time series from Toscana\n",
    "# First we can check our files within the folder:\n",
    "path =r'data/streamflow/raw_data/IT/TOS'\n",
    "filenames = glob.glob(path + \"/*.csv\")\n",
    "\n",
    "# Initialize a DataFrame to store the final time series data with dates as index:\n",
    "timeseries_ITTO = pd.DataFrame(index=pd.date_range('01-01-1920', '09-30-2023', freq='D'))\n",
    "\n",
    "# Iterate through each file and process its contents:\n",
    "for filename in tqdm.tqdm(filenames):\n",
    "    # Read the CSV file containing the time series data:\n",
    "    timeseries_station = pd.read_csv(filename, encoding='latin-1', skiprows=19, delimiter=\";\", \n",
    "                                     names=[\"dates\", \"Q_m3_s\", \"Val\"], decimal=',')\n",
    "\n",
    "    # Convert the 'dates' column to datetime format and set it as index:\n",
    "    timeseries_station[\"dates\"] = pd.to_datetime(timeseries_station[\"dates\"], format='%d/%m/%Y')\n",
    "    timeseries_station.set_index(\"dates\", inplace=True)\n",
    "    \n",
    "    # Extract the station name from the file:\n",
    "    station = pd.read_csv(filename, encoding='latin-1', skiprows=0, delimiter=\";\", nrows=1)\n",
    "    station = station.iloc[:, 1]\n",
    "    \n",
    "    # Resample the data to fill any missing dates and compute daily means:\n",
    "    timeseries_station = timeseries_station.resample('D').mean()\n",
    "    \n",
    "    # Store the time series data for the station in the final DataFrame:\n",
    "    timeseries_ITTO.loc[:, station[0]] = timeseries_station.Q_m3_s\n",
    "    \n",
    "# Check the final time series:\n",
    "timeseries_ITTO.head() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "246e8e19",
   "metadata": {},
   "source": [
    "- ISPRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c3a1a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Italian time series from ISPRA\n",
    "# First we can check our files within the folder:\n",
    "path =r'data/streamflow/raw_data/IT/ISPRA'\n",
    "filenames = glob.glob(path + \"/*.csv\")\n",
    "\n",
    "# Initialize a DataFrame to store the final time series data with dates as index:\n",
    "timeseries_ITIS = pd.DataFrame(index=pd.date_range('1900-01-01', '2022-12-31', freq='D'))\n",
    "\n",
    "# Iterate through each file and process its contents:\n",
    "for filename in tqdm.tqdm(filenames):\n",
    "    # Read the CSV file containing the time series data:\n",
    "    timeseries_filename = pd.read_csv(filename, encoding='latin-1', header=0, delimiter=\",\", \n",
    "                                      decimal=\".\", names=[\"dates\", \"Q_m3s\", \"Quality\"])\n",
    "    \n",
    "    # Replace negative values in 'Q_m3s' column with NaN:\n",
    "    timeseries_filename.Q_m3s[timeseries_filename.Q_m3s < 0] = np.nan\n",
    "    \n",
    "    # Convert the 'dates' column to datetime format:\n",
    "    timeseries_filename[\"dates\"] = pd.to_datetime(timeseries_filename[\"dates\"], format='%Y-%m-%dT%H:%M:%S')\n",
    "    \n",
    "    # Set 'dates' column as index:\n",
    "    timeseries_filename.set_index(\"dates\", inplace=True)\n",
    "    \n",
    "    # Resample the data to daily intervals, taking the mean of each day:\n",
    "    timeseries_filename = timeseries_filename.resample('D').mean()\n",
    "    \n",
    "    # Extract the station name from the filename:\n",
    "    namestation = os.path.basename(filename)\n",
    "    namestation = namestation.replace(\".csv\", \"\")\n",
    "    namestation = namestation.replace(\"_\", \":\")\n",
    "    \n",
    "    # Store the time series data for the station in the final DataFrame:\n",
    "    timeseries_ITIS.loc[:, namestation] = timeseries_filename.Q_m3s\n",
    "\n",
    "# Check the final time series:\n",
    "timeseries_ITIS.head() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ef16e6a",
   "metadata": {},
   "source": [
    "### 12. Poland"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93cad947",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Polish daily streamflow files\n",
    "\n",
    "# First you should upload the network information file\n",
    "network_PL = pd.read_csv(r'data/streamflow/raw_data/PL/network/lista_stacji_hydro.csv', encoding='latin-1', header = None,\n",
    "                        names = [\"code\", \"name\", \"river\", \"col4\"])\n",
    "\n",
    "# Now we upload the streamflow data\n",
    "path =r'data/streamflow/raw_data/PL'\n",
    "filenames = glob.glob(path + \"/*.csv\")\n",
    "\n",
    "# Here we create an empty dataframe to be populated with the streamflow data\n",
    "timeseries_PL = pd.DataFrame(index = pd.date_range('01-01-1950','12-31-2022', freq='D'), columns = network_PL.code)\n",
    "\n",
    "# We make one loop to cover each month avaialble in the dataset\n",
    "for filename in tqdm.tqdm(filenames):\n",
    "    \n",
    "    # Now we read the data for the specific month\n",
    "    timeseries_month = pd.read_csv(filename, encoding='latin-1', delimiter = \",\", header = None, names = [\"code\", \"name\", \"river\", \n",
    "                                                                                                          \"year_hydro\", \"month_hydro\", \n",
    "                                                                                                          \"day\", \"level\", \"flow\", \n",
    "                                                                                                          \"temperature\", \"month\"])\n",
    "\n",
    "    # Pay attention that we read the hydrological year (not the calendar), so we should convert it before\n",
    "    # Apply the condition and update the 'year' column according to the calendar month (hydrological year starts in November)\n",
    "    timeseries_month['year'] = timeseries_month['year_hydro']\n",
    "    timeseries_month.loc[timeseries_month['month'] >= 11, 'year'] = timeseries_month['year_hydro'] - 1\n",
    "    \n",
    "    # Create a new column with the dates (converted to datetime)\n",
    "    timeseries_month[\"date\"] = pd.to_datetime(timeseries_month[['year', 'month', 'day']])\n",
    "\n",
    "    # Here we retrieve the stations in this month (unique ones)\n",
    "    stations_this_month = timeseries_month.code.unique().tolist()\n",
    "\n",
    "    # Here we make a loop to retrieve the data for the specific month for each station\n",
    "    for station in stations_this_month:\n",
    "        timeseries_station = timeseries_month[timeseries_month.code == station]\n",
    "        timeseries_station.set_index(\"date\", inplace=True)\n",
    "        timeseries_PL.loc[timeseries_station.index, station] = timeseries_station.flow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9565ff81",
   "metadata": {},
   "source": [
    "### 13. Portugal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55efc38b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Portuguese daily streamflow files\n",
    "# First we can check our files within the folder. For Portugal, we can download the \n",
    "# data in groups of maximum (10 gauges) according to the record numbers, so the code takes\n",
    "# care of the proper aggregation regardless of how the csvs are organized:\n",
    "path =r'data/streamflow/raw_data/PT'\n",
    "filenames = glob.glob(path + \"/*.csv\")\n",
    "\n",
    "# Iterate through each file and process its contents:\n",
    "i = 0\n",
    "for filename in tqdm.tqdm(filenames):\n",
    "    \n",
    "    # There are some useless values at the end after a blank line. As it varies from file to file, we apply the following \n",
    "    # methodology to get rid of them:\n",
    "    \n",
    "    tsdf = pd.read_csv(filename, encoding='latin-1', skiprows=2, engine='python', skip_blank_lines=False)\n",
    "    blank_df = tsdf.loc[tsdf.isnull().all(1)]\n",
    "    if len(blank_df) > 0:\n",
    "        first_blank_index = blank_df.index[0]\n",
    "        tsdf = tsdf[:first_blank_index]\n",
    "    \n",
    "    # We also need to delete the first row, and all the columns that contain 'Unnamed':\n",
    "    tsdf = tsdf.drop([0])\n",
    "    tsdf = tsdf.drop(tsdf.columns[tsdf.columns.str.contains(pat = 'Unnamed')].tolist(), axis = 1)\n",
    "\n",
    "    # Now we convert our dataset column that will be used as index to datetime:\n",
    "    tsdf[\"dates\"] = pd.to_datetime(tsdf['DATA'], format='%d/%m/%Y %H:%M')\n",
    "    tsdf = tsdf.drop(\"DATA\", axis = 1)\n",
    "    tsdf.set_index('dates', inplace = True)\n",
    "\n",
    "    # There are rows missing when there are gaps, therefore this is a nice way of overcame this and have our time-series \n",
    "    # range completed:\n",
    "    timeseries = pd.DataFrame(index = pd.date_range('01-01-1900','12-31-2023', freq='D'), data = tsdf.astype('float'))\n",
    "    \n",
    "    # Now the concatatanion is made, and at the end we might have the total of streamflow gauges as columns:\n",
    "    if i == 0:\n",
    "        timeseries_PT = timeseries\n",
    "    else:\n",
    "        timeseries_PT = pd.concat([timeseries_PT, timeseries], axis=1)\n",
    "    i = i + 1\n",
    "\n",
    "# Check the final time series:\n",
    "timeseries_PT.head()       "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36425cc5",
   "metadata": {},
   "source": [
    "### 14. Slovenia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a716c871",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Slovenian time series from OPW\n",
    "# First we can check our files within the folder:\n",
    "path =r'data/streamflow/raw_data/SI'\n",
    "filenames = glob.glob(path + \"/*.xls\")\n",
    "\n",
    "timeseries_SI = pd.DataFrame(index = pd.date_range('01-01-1950','12-31-2022', freq='D'))\n",
    "\n",
    "# Iterate through each file and process its contents:\n",
    "for filename in tqdm.tqdm(filenames):\n",
    "    # First we read our time-series:\n",
    "    data = pd.read_excel(filename, sheet_name = 0, usecols = [\"Datum\", \"pretok (m3/s)\"])\n",
    "\n",
    "    # We define new column names:\n",
    "    names = ['dates', 'Qm3s']\n",
    "    data.columns = names\n",
    "\n",
    "    # Convert our column of dates to datetime format:\n",
    "    data[\"dates\"] = pd.to_datetime(data[\"dates\"], format='%d.%m.%Y')\n",
    "\n",
    "    # Here we can set the index as the dates column:\n",
    "    data.set_index(\"dates\", inplace = True)\n",
    "\n",
    "    # First we can retrieve the station name:\n",
    "    namestation = os.path.basename(filename)\n",
    "    namestation = namestation.split(\"_\", 4)[1]\n",
    "    timeseries_SI.loc[:, int(namestation)] = data.Qm3s\n",
    "\n",
    "# Check the final time series:\n",
    "timeseries_SI.head()       "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a257b656",
   "metadata": {},
   "source": [
    "## Exporting the concatenated data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beffcbbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After the concatenation of the desired datasets, users can simply export the file as:\n",
    "PATH_OUTPUT = \"PATH_TO_WHERE_YOU_WANT_TO_EXPORT\"\n",
    "timeseries_AT.to_csv(PATH_OUTPUT+\"timeseries_AT.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b608e26e",
   "metadata": {},
   "source": [
    "# End"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
