{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46e94d6f",
   "metadata": {},
   "source": [
    "# Complementary extra codes: Concatenation of new catchments\n",
    "\n",
    "\n",
    "Author: Thiago Nascimento (thiago.nascimento@eawag.ch)\n",
    "\n",
    "This notebook complements the EStreams publication and can be used to concatenate catchment attributes derived from new basins into the original dataset. This might be useful for users that wish to extend their datasets to new areas, for example. \n",
    "\n",
    "\n",
    "* Note that this code enables not only the replicability of the current database but also the extrapolation to new catchment areas. \n",
    "* Additionally, the user should download and insert the original raw-data in the folder of the same name prior to run this code. \n",
    "* The original third-party data used were not made avaialble in this repository due to redistribution and storage-space reasons.  \n",
    "\n",
    "## Requirements\n",
    "**Python:**\n",
    "\n",
    "* Python>=3.6\n",
    "* Jupyter\n",
    "* geopandas=0.10.2\n",
    "* numpy\n",
    "* os\n",
    "* pandas\n",
    "* tqdm\n",
    "* warnings\n",
    "\n",
    "Check the Github repository for an environment.yml (for conda environments) or requirements.txt (pip) file.\n",
    "\n",
    "**Files:**\n",
    "\n",
    "* data/concatenation_new_basins/old_data/*.csv\n",
    "* data/concatenation_new_basins/new_data/*.csv\n",
    "\n",
    "**Directory:**\n",
    "\n",
    "* Clone the GitHub directory locally\n",
    "* Place any third-data variables in their respective directory.\n",
    "* ONLY update the \"PATH\" variable in the section \"Configurations\", with their relative path to the EStreams directory. \n",
    "\n",
    "## Observations\n",
    "\n",
    "* This notebook assumes that all the \"new\" and \"old\" catchments have their attributes exported and stored correctly in their respective folders. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f7648ab",
   "metadata": {},
   "source": [
    "# Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f6e053",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tqdm as tqdm\n",
    "import os\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b31c9d67",
   "metadata": {},
   "source": [
    "# Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13078a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only editable variables\n",
    "# Relative path to your local directory\n",
    "PATH = \"../../..\"\n",
    "\n",
    "# Suppress all warnings:\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f5fc4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Non-editable variables:\n",
    "PATH_OUTPUT = \"results/\"\n",
    "path_old_files = 'data/concatenation_new_basins/old_files/'\n",
    "path_new_files = 'data/concatenation_new_basins/new_files/'\n",
    "\n",
    "# Set the directory:\n",
    "os.chdir(PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4909fe84",
   "metadata": {},
   "source": [
    "# Import data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4644d225",
   "metadata": {},
   "source": [
    "## Static attributes\n",
    "* Here we automatize the concatenation of the new dataset into the old ones.\n",
    "* Note that the concatenation is done in pairs (old and new)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f41d0ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Static datasets names:\n",
    "datasets_static = ['estreams_lithology_attributes.csv',\n",
    "    'estreams_hydrology_attributes.csv',\n",
    "    'estreams_landcover_attributes.csv',\n",
    "    'estreams_meteorology_density.csv',\n",
    "    'estreams_snowcover_attributes.csv',\n",
    "    'estreams_soil_attributes.csv',\n",
    "    'estreams_terrain_attributes.csv',\n",
    "    'estreams_vegetation_attributes.csv']\n",
    "\n",
    "# Output filename options:\n",
    "datasets_static_output_options = {\n",
    "    'estreams_lithology_attributes.csv': PATH_OUTPUT+\"/staticattributes/estreams_lithology_attributes.csv\",\n",
    "    'estreams_hydrology_attributes.csv': PATH_OUTPUT+\"/staticattributes/estreams_hydrology_attributes.csv\",\n",
    "    'estreams_landcover_attributes.csv': PATH_OUTPUT+\"/staticattributes/estreams_landcover_attributes.csv\",\n",
    "    'estreams_meteorology_density.csv': PATH_OUTPUT+\"/staticattributes/estreams_meteorology_density.csv\",\n",
    "    'estreams_snowcover_attributes.csv': PATH_OUTPUT+\"/staticattributes/estreams_snowcover_attributes.csv\",\n",
    "    'estreams_soil_attributes.csv': PATH_OUTPUT+\"/staticattributes/estreams_soil_attributes.csv\",\n",
    "    'estreams_terrain_attributes.csv': PATH_OUTPUT+\"/staticattributes/estreams_terrain_attributes.csv\",\n",
    "    'estreams_vegetation_attributes.csv': PATH_OUTPUT+\"/staticattributes/estreams_vegetation_attributes.csv\"\n",
    "    }\n",
    "\n",
    "for chosen_dataset in tqdm.tqdm(datasets_static):\n",
    "    \n",
    "    # If the file is not in the folder (or stored with a different name), the run will skip this dataset. \n",
    "    try: \n",
    "        # Get the old, new and output pathnames\n",
    "        csv_file_old = path_old_files+chosen_dataset\n",
    "        csv_file_new = path_new_files+chosen_dataset\n",
    "        csv_output = datasets_static_output_options.get(chosen_dataset)\n",
    "        \n",
    "        # Read the old and new datasets:\n",
    "        estreams_old = pd.read_csv(csv_file_old, index_col=0)\n",
    "        estreams_new = pd.read_csv(csv_file_new, index_col=0)\n",
    "        \n",
    "        # Concatenate and sort it by index\n",
    "        estreams_concatenated = pd.concat([estreams_old, estreams_new], axis=0)\n",
    "        estreams_concatenated = estreams_concatenated.sort_index(axis=0)\n",
    "        \n",
    "        # Delete some specific characters (in this case RS and ISGR)\n",
    "        estreams_concatenated = estreams_concatenated.filter(regex='^(?!.*(ISGR|RS00)).*$', axis=0)\n",
    "        \n",
    "        # Here we adjust the lithology, because it was saving (original) only for the classes present:\n",
    "        if chosen_dataset == \"estreams_lithology_attributes.csv\":\n",
    "            estreams_concatenated = estreams_concatenated[[\"lit_fra_ev\", \"lit_fra_ig\", \"lit_fra_mt\", \"lit_fra_nd\", \"lit_fra_pa\", \"lit_fra_pb\", \n",
    "                                                           \"lit_fra_pi\", \"lit_fra_py\", \"lit_fra_sc\", \"lit_fra_sm\", \"lit_fra_ss\", \"lit_fra_su\",\n",
    "                                                           \"lit_fra_va\", \"lit_fra_vb\", \"lit_fra_vi\", \"lit_fra_wb\", \"lit_dom\", \"tot_area\", \"bedrk_dep\"]]\n",
    "            # Fill NaN values with 0 in the first 16 columns\n",
    "            estreams_concatenated.iloc[:, :16] = estreams_concatenated.iloc[:, :16].fillna(0)                                                 \n",
    "        else:\n",
    "            1+1\n",
    "\n",
    "       # Here we also adjust the hydrology:\n",
    "        if chosen_dataset == \"estreams_hydrology_attributes.csv\":\n",
    "            estreams_concatenated.iloc[:, -2:] = estreams_concatenated.iloc[:, -2:].fillna(0)                                                 \n",
    "        else:\n",
    "            1+1\n",
    "            \n",
    "        # Save the data:\n",
    "        estreams_concatenated.to_csv(csv_output)\n",
    "        \n",
    "        # Prints usefull to visualize what was done:\n",
    "        print(\"Dataset:\", chosen_dataset)\n",
    "        print(\"Number of catchments in the old file:\", len(estreams_old))\n",
    "        print(\"Number of catchments in the new file:\", len(estreams_new))\n",
    "        \n",
    "    except:\n",
    "        print(\"The dataset\", chosen_dataset, \"is not avaialble in the folders.\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a4007c",
   "metadata": {},
   "source": [
    "## Temporal attributes\n",
    "* Here we automatize the concatenation of the new dataset into the old ones.\n",
    "* Note that the concatenation is done in pairs (old and new).\n",
    "* The meteorological time-series are not considered at this part. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9855a557",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temporal datasets names:\n",
    "datasets_temporal = [\n",
    "    'estreams_irrigation_yearly.csv',\n",
    "    'estreams_LAI_monhtly.csv',\n",
    "    'estreams_LAI_yearly.csv',\n",
    "    'estreams_NDVI_monhtly.csv',\n",
    "    'estreams_NDVI_yearly.csv',\n",
    "    'estreams_snowcover_monhtly.csv',\n",
    "    'estreams_snowcover_yearly.csv']\n",
    "\n",
    "# Output filename options:\n",
    "datasets_temporal_output_options = {\n",
    "    'estreams_irrigation_yearly.csv': PATH_OUTPUT+\"/timeseries/irrigation/estreams_irrigation_yearly.csv\",\n",
    "    'estreams_LAI_monhtly.csv': PATH_OUTPUT+\"/timeseries/vegetationindices/estreams_LAI_monhtly.csv\",\n",
    "    'estreams_LAI_yearly.csv': PATH_OUTPUT+\"/timeseries/vegetationindices/estreams_LAI_yearly.csv\",\n",
    "    'estreams_NDVI_monhtly.csv': PATH_OUTPUT+\"/timeseries/vegetationindices/estreams_NDVI_monhtly.csv\",\n",
    "    'estreams_NDVI_yearly.csv': PATH_OUTPUT+\"/timeseries/vegetationindices/estreams_NDVI_yearly.csv\",\n",
    "    'estreams_snowcover_monhtly.csv': PATH_OUTPUT+\"/timeseries/snowcover/estreams_snowcover_monhtly.csv\",\n",
    "    'estreams_snowcover_yearly.csv': PATH_OUTPUT+\"/timeseries/snowcover/estreams_snowcover_yearly.csv\",\n",
    "    }\n",
    "\n",
    "for chosen_dataset in tqdm.tqdm(datasets_temporal):\n",
    "    \n",
    "    # If the file is not in the folder (or stored with a different name), the run will skip this dataset. \n",
    "    try: \n",
    "        # Get the old, new and output pathnames\n",
    "        csv_file_old = path_old_files+chosen_dataset\n",
    "        csv_file_new = path_new_files+chosen_dataset\n",
    "        csv_output = datasets_temporal_output_options.get(chosen_dataset)\n",
    "        \n",
    "        # Read the old and new datasets:\n",
    "        estreams_old = pd.read_csv(csv_file_old, index_col=0)\n",
    "        estreams_new = pd.read_csv(csv_file_new, index_col=0)\n",
    "        \n",
    "        if chosen_dataset != 'estreams_irrigation_yearly.csv':\n",
    "            \n",
    "            # Here we set the index to datetime (avoid problems during concatenation):\n",
    "            try:\n",
    "                estreams_old.index = pd.to_datetime(estreams_old.index)\n",
    "            except:\n",
    "                estreams_old.index = pd.to_datetime(estreams_old.index, format='%d.%m.%Y')\n",
    "            try:\n",
    "                estreams_new.index = pd.to_datetime(estreams_new.index)\n",
    "            except:\n",
    "                estreams_new.index = pd.to_datetime(estreams_new.index, format='%d.%m.%Y')\n",
    "        else:\n",
    "            1+1\n",
    "            \n",
    "        # Concatenate and sort it by index\n",
    "        estreams_concatenated = pd.concat([estreams_old, estreams_new], axis=1)\n",
    "        estreams_concatenated = estreams_concatenated.sort_index(axis=1)\n",
    "        \n",
    "        ## Delete some specific characters (in this case RS and ISGR)\n",
    "        #estreams_concatenated = estreams_concatenated.filter(regex='^(?!.*(ISGR|RS00)).*$', axis=1)\n",
    "        \n",
    "        # Save the data:\n",
    "        estreams_concatenated.to_csv(csv_output)\n",
    "        \n",
    "        # Prints usefull to visualize what was done:\n",
    "        print(\"Dataset:\", chosen_dataset)\n",
    "        print(\"Number of catchments in the old file:\", estreams_old.shape[1])\n",
    "        print(\"Number of catchments in the new file:\", estreams_new.shape[1])\n",
    "        \n",
    "    except:\n",
    "        print(\"The dataset\", chosen_dataset, \"is not avaialble in the folders.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a08b6940",
   "metadata": {},
   "source": [
    "## Meteorological csv-data (PET, P and T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fb135a6",
   "metadata": {},
   "source": [
    "### PET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a4b625d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the old and new datasets:\n",
    "estreams_old = pd.read_csv(path_old_files+\"estreams_meteorology_pet.csv\", index_col=0)\n",
    "estreams_new = pd.read_csv(path_new_files+\"estreams_meteorology_pet.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc001905",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we set the index to datetime (avoid problems during concatenation):\n",
    "try:\n",
    "    estreams_old.index = pd.to_datetime(estreams_old.index)\n",
    "except:\n",
    "    estreams_old.index = pd.to_datetime(estreams_old.index, format='%d.%m.%Y')\n",
    "try:\n",
    "    estreams_new.index = pd.to_datetime(estreams_new.index)\n",
    "except:\n",
    "    estreams_new.index = pd.to_datetime(estreams_new.index, format='%d.%m.%Y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "894578c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate and sort it by index\n",
    "estreams_concatenated = pd.concat([estreams_old, estreams_new], axis=1, ignore_index=False)\n",
    "estreams_concatenated = estreams_concatenated.sort_index(axis=1)\n",
    "estreams_concatenated"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "472502c3",
   "metadata": {},
   "source": [
    "- (Extra) Example of how to delete specific basins from the analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0771141e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Filter the DataFrame to keep columns that do not contain 'ISGR' or 'RS00' in their names\n",
    "#estreams_concatenated = estreams_concatenated.filter(regex='^(?!.*(ISGR|RS00)).*$', axis=1)\n",
    "#estreams_concatenated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea4df24f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the data:\n",
    "estreams_concatenated.to_csv(\"results/timeseries/meteorology/estreams_meteorology_pet.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ca9bfde",
   "metadata": {},
   "source": [
    "### P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdff502c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the old and new datasets:\n",
    "estreams_old = pd.read_csv(path_old_files+\"estreams_meteorology_precipitation.csv\", index_col=0)\n",
    "estreams_new = pd.read_csv(path_new_files+\"estreams_meteorology_precipitation.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f9a585b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we set the index to datetime (avoid problems during concatenation):\n",
    "try:\n",
    "    estreams_old.index = pd.to_datetime(estreams_old.index)\n",
    "except:\n",
    "    estreams_old.index = pd.to_datetime(estreams_old.index, format='%d.%m.%Y')\n",
    "try:\n",
    "    estreams_new.index = pd.to_datetime(estreams_new.index)\n",
    "except:\n",
    "    estreams_new.index = pd.to_datetime(estreams_new.index, format='%d.%m.%Y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf44e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate and sort it by index\n",
    "estreams_concatenated = pd.concat([estreams_old, estreams_new], axis=1, ignore_index=False)\n",
    "estreams_concatenated = estreams_concatenated.sort_index(axis=1)\n",
    "estreams_concatenated"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeca4bce",
   "metadata": {},
   "source": [
    "- (Extra) Example of how to delete specific basins from the analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc4ad33f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Filter the DataFrame to keep columns that do not contain 'ISGR' or 'RS00' in their names\n",
    "#estreams_concatenated = estreams_concatenated.filter(regex='^(?!.*(ISGR|RS00)).*$', axis=1)\n",
    "#estreams_concatenated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "977eb085",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the data:\n",
    "estreams_concatenated.to_csv(\"results/timeseries/meteorology/estreams_meteorology_precipitation.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10bdd987",
   "metadata": {},
   "source": [
    "### T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01cb6227",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the old and new datasets:\n",
    "estreams_old = pd.read_csv(path_old_files+\"estreams_meteorology_temperature.csv\", index_col=0)\n",
    "estreams_new = pd.read_csv(path_new_files+\"estreams_meteorology_temperature.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bcc7db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we set the index to datetime (avoid problems during concatenation):\n",
    "try:\n",
    "    estreams_old.index = pd.to_datetime(estreams_old.index)\n",
    "except:\n",
    "    estreams_old.index = pd.to_datetime(estreams_old.index, format='%d.%m.%Y')\n",
    "try:\n",
    "    estreams_new.index = pd.to_datetime(estreams_new.index)\n",
    "except:\n",
    "    estreams_new.index = pd.to_datetime(estreams_new.index, format='%d.%m.%Y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5a89d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate and sort it by index\n",
    "estreams_concatenated = pd.concat([estreams_old, estreams_new], axis=1, ignore_index=False)\n",
    "estreams_concatenated = estreams_concatenated.sort_index(axis=1)\n",
    "estreams_concatenated"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d62a689",
   "metadata": {},
   "source": [
    "- (Extra) Example of how to delete specific basins from the analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9adfe95",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Filter the DataFrame to keep columns that do not contain 'ISGR' or 'RS00' in their names\n",
    "#estreams_concatenated = estreams_concatenated.filter(regex='^(?!.*(ISGR|RS00)).*$', axis=1)\n",
    "#estreams_concatenated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38feff13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the data:\n",
    "estreams_concatenated.to_csv(\"results/timeseries/meteorology/estreams_meteorology_temperature.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cfa5cd2",
   "metadata": {},
   "source": [
    "# End"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
