{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46e94d6f",
   "metadata": {},
   "source": [
    "# Complementary extra codes: Concatenation of new catchments\n",
    "\n",
    "\n",
    "Author: Thiago Nascimento (thiago.nascimento@eawag.ch)\n",
    "\n",
    "This notebook complements the EStreams publication and can be used to concatenate catchment attributes derived from new basins into the original dataset. This might be useful for users that wish to extend their datasets to new areas, for example. \n",
    "\n",
    "\n",
    "* Note that this code enables not only the replicability of the current database but also the extrapolation to new catchment areas. \n",
    "* Additionally, the user should download and insert the original raw-data in the folder of the same name prior to run this code. \n",
    "* The original third-party data used were not made avaialble in this repository due to redistribution and storage-space reasons.  \n",
    "\n",
    "## Requirements\n",
    "**Python:**\n",
    "\n",
    "* Python>=3.6\n",
    "* Jupyter\n",
    "* geopandas=0.10.2\n",
    "* numpy\n",
    "* os\n",
    "* pandas\n",
    "* tqdm\n",
    "* warnings\n",
    "\n",
    "Check the Github repository for an environment.yml (for conda environments) or requirements.txt (pip) file.\n",
    "\n",
    "**Files:**\n",
    "\n",
    "* data/concatenation_new_basins/old_data/*.csv\n",
    "* data/concatenation_new_basins/new_data/*.csv\n",
    "\n",
    "**Directory:**\n",
    "\n",
    "* Clone the GitHub directory locally\n",
    "* Place any third-data variables in their respective directory.\n",
    "* ONLY update the \"PATH\" variable in the section \"Configurations\", with their relative path to the EStreams directory. \n",
    "\n",
    "## Observations\n",
    "\n",
    "* This notebook assumes that all the \"new\" and \"old\" catchments have their attributes exported and stored correctly in their respective folders. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f7648ab",
   "metadata": {},
   "source": [
    "# Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f6e053",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tqdm as tqdm\n",
    "import os\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b31c9d67",
   "metadata": {},
   "source": [
    "# Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13078a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only editable variables\n",
    "# Relative path to your local directory\n",
    "PATH = \"../../..\"\n",
    "\n",
    "# Suppress all warnings:\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f5fc4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Non-editable variables:\n",
    "PATH_OUTPUT = \"results/\"\n",
    "path_old_files = 'data/update_old_basins/old_files/'\n",
    "path_new_files = 'data/update_old_basins/new_files/'\n",
    "\n",
    "# Set the directory:\n",
    "os.chdir(PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4909fe84",
   "metadata": {},
   "source": [
    "# Import data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4644d225",
   "metadata": {},
   "source": [
    "## Static attributes\n",
    "* Here we automatize the update of the new dataset into the old ones.\n",
    "* Note that the update is done in pairs (old and new)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f41d0ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Static datasets names:\n",
    "datasets_static = ['estreams_hydrometeo_signatures.csv',\n",
    "    ]\n",
    "\n",
    "# Output filename options:\n",
    "datasets_static_output_options = {\n",
    "    'estreams_hydrometeo_signatures.csv': PATH_OUTPUT+\"/staticattributes/estreams_hydrometeo_signatures.csv\",\n",
    "    }\n",
    "\n",
    "for chosen_dataset in tqdm.tqdm(datasets_static):\n",
    "    \n",
    "    # If the file is not in the folder (or stored with a different name), the run will skip this dataset. \n",
    "    try: \n",
    "        # Get the old, new and output pathnames\n",
    "        csv_file_old = path_old_files+chosen_dataset\n",
    "        csv_file_new = path_new_files+chosen_dataset\n",
    "        csv_output = datasets_static_output_options.get(chosen_dataset)\n",
    "        \n",
    "        # Read the old and new datasets:\n",
    "        estreams_old = pd.read_csv(csv_file_old, index_col=0)\n",
    "        estreams_new = pd.read_csv(csv_file_new, index_col=0)\n",
    "        \n",
    "        # Make a copy to avoid overwriting original\n",
    "        estreams_updated = estreams_old.copy()\n",
    "\n",
    "        # Update only overlapping values\n",
    "        estreams_updated.update(estreams_new)\n",
    "\n",
    "        # Rows in old that have no match in new\n",
    "        not_in_new = estreams_old.index.difference(estreams_new.index)\n",
    "\n",
    "        # Rows in old that had a match but were unchanged\n",
    "        common_indices = estreams_old.index.intersection(estreams_new.index)\n",
    "        unchanged_common = [idx for idx in common_indices if estreams_old.loc[idx].equals(estreams_updated.loc[idx])]\n",
    "\n",
    "        # Combine both sets\n",
    "        not_updated_indices = list(not_in_new) + unchanged_common\n",
    "\n",
    "        # Get full DataFrame\n",
    "        not_updated_rows = estreams_old.loc[not_updated_indices].index.tolist()\n",
    "\n",
    "        print(\"\\nTotal rows in old that were NOT updated (no match or identical):\", len(not_updated_rows))\n",
    "        print(not_updated_rows)\n",
    "\n",
    "        # Save the data:\n",
    "        estreams_updated.to_csv(csv_output)\n",
    "        \n",
    "        # Prints usefull to visualize what was done:\n",
    "        print(\"Dataset:\", chosen_dataset)\n",
    "        print(\"Number of catchments in the old file:\", len(estreams_old))\n",
    "        print(\"Number of catchments in the new file:\", len(estreams_new))\n",
    "        \n",
    "    except:\n",
    "        print(\"The dataset\", chosen_dataset, \"is not avaialble in the folders.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a4007c",
   "metadata": {},
   "source": [
    "## Temporal attributes\n",
    "* Here we automatize the update of the new dataset into the old ones.\n",
    "* Note that the update is done in pairs (old and new).\n",
    "* The meteorological time-series are not considered at this part. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d90e404",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "base_old = 'data/update_old_basins/old_files/streamflowindices'\n",
    "base_new = 'data/update_old_basins/new_files/streamflowindices'\n",
    "base_output = 'results/timeseries/streamflowindices'\n",
    "\n",
    "# Subfolders to loop through\n",
    "subfolders = ['monthly', 'seasonal', 'weekly', 'yearly']\n",
    "\n",
    "for subfolder in subfolders:\n",
    "    old_dir = os.path.join(base_old, subfolder)\n",
    "    new_dir = os.path.join(base_new, subfolder)\n",
    "    output_dir = os.path.join(base_output, subfolder)\n",
    "\n",
    "    # Skip if the new subfolder doesn't exist\n",
    "    if not os.path.isdir(new_dir):\n",
    "        continue\n",
    "\n",
    "    # Process each CSV in the new folder\n",
    "    for file in os.listdir(new_dir):\n",
    "        if file.endswith('.csv'):\n",
    "            new_file_path = os.path.join(new_dir, file)\n",
    "            old_file_path = os.path.join(old_dir, file)\n",
    "            output_file_path = os.path.join(output_dir, file)\n",
    "\n",
    "            # Only update if the file exists in old folder\n",
    "            if os.path.exists(old_file_path):\n",
    "                # Load both DataFrames\n",
    "                df_old = pd.read_csv(old_file_path, index_col=0, parse_dates=True)\n",
    "                df_new = pd.read_csv(new_file_path, index_col=0, parse_dates=True)\n",
    "\n",
    "                # Update the old DataFrame with the new values\n",
    "                df_old.update(df_new)\n",
    "\n",
    "                # Save the updated DataFrame, overwriting the old file\n",
    "                df_old.to_csv(output_file_path)\n",
    "\n",
    "                print(f\"Updated: {subfolder}/{file}\")\n",
    "            else:\n",
    "                print(f\"Skipped: {subfolder}/{file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c6fde64",
   "metadata": {},
   "source": [
    "## Small check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "402c2a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_old = pd.read_csv('data/update_old_basins/old_files/streamflowindices/monthly/monthly_streamflow_mean.csv', index_col=0, parse_dates=True)\n",
    "df_new = pd.read_csv('data/update_old_basins/new_files/streamflowindices/monthly/monthly_streamflow_mean.csv', index_col=0, parse_dates=True)\n",
    "df_updated = pd.read_csv('results/timeseries/streamflowindices/monthly/monthly_streamflow_mean.csv', index_col=0, parse_dates=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a7c818",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_updated.loc[:, \"DENW1234\"].plot()\n",
    "df_new.loc[:, \"DENW1234\"].plot(alpha=0.5)\n",
    "df_old.loc[:, \"DENW1234\"].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "061c451b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_updated.loc[:, \"AT000001\"].plot()\n",
    "df_old.loc[:, \"AT000001\"].plot(alpha=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cfa5cd2",
   "metadata": {},
   "source": [
    "# End"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "estreams",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
