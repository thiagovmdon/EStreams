{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "20d673c9",
   "metadata": {},
   "source": [
    "# Complementary extra codes: List unique catchments\n",
    "\n",
    "Author: Thiago Nascimento (thiago.nascimento@eawag.ch)\n",
    "\n",
    "This notebook complements the EStreams publication and can be used to filter potential duplicated catchments within the dataset. The approach takes as input the 'estreams_gauging_stations.csv' file and retrieve a final list with only _unique_ catchments mantaining the ones with the _longest_ time series of records, when duplicated exists. \n",
    "\n",
    "* Note that this code enables not only the replicability of the current database but also the extrapolation to new catchment areas. \n",
    "* Additionally, the user should download and insert the original raw-data in the folder of the same name prior to run this code. \n",
    "* The original third-party data used were not made available in this repository due to redistribution and storage-space reasons.  \n",
    "\n",
    "## Requirements\n",
    "**Python:**\n",
    "\n",
    "* ast\n",
    "* Python>=3.6\n",
    "* Jupyter\n",
    "* os\n",
    "* pandas\n",
    "* warnings\n",
    "\n",
    "Check the Github repository for an environment.yml (for conda environments) or requirements.txt (pip) file.\n",
    "\n",
    "**Files:**\n",
    "\n",
    "* data/streamflow/estreams_gauging_stations.csv\n",
    "\n",
    "**Directory:**\n",
    "\n",
    "* Clone the GitHub directory locally\n",
    "* Place any third-data variables in their respective directory.\n",
    "* ONLY update the \"PATH\" variable in the section \"Configurations\", with their relative path to the EStreams directory. \n",
    "\n",
    "## Observations\n",
    "- It may be possible that user do not want the gauge with the longest time series, but rather the most recent one, for example. In this case, they would need to adjust the current code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56f627dd",
   "metadata": {},
   "source": [
    "# Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ffa447",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import warnings\n",
    "import ast"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dab82684",
   "metadata": {},
   "source": [
    "\n",
    "# Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "436b7673",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Relative path to your local directory\n",
    "PATH = \"../../..\"\n",
    "#PATH = r\"/Users/thiagomedeirosdonascimento/Library/CloudStorage/OneDrive-Personal/PhD/Eawag/Papers/Paper1_Database/Database/EStreams/\"\n",
    "\n",
    "# Set the directory:\n",
    "os.chdir(PATH)\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=Warning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "004092f5",
   "metadata": {},
   "source": [
    "# Import data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63cbadcf",
   "metadata": {},
   "source": [
    "### - Network information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c25b0a9f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "network_estreams = pd.read_csv('data/streamflow/estreams_gauging_stations.csv', encoding='utf-8')\n",
    "network_estreams.set_index(\"basin_id\", inplace = True)\n",
    "\n",
    "# Convert 'date_column' and 'time_column' to datetime\n",
    "network_estreams['start_date'] = pd.to_datetime(network_estreams['start_date'])\n",
    "network_estreams['end_date'] = pd.to_datetime(network_estreams['end_date'])\n",
    "\n",
    "# Here we adjust the duplicated_suspect and nested_catchments columns to help our dealing with them:\n",
    "network_estreams[\"duplicated_suspect\"][network_estreams[\"duplicated_suspect\"].notna()] = network_estreams[\"duplicated_suspect\"][network_estreams[\"duplicated_suspect\"].notna()].apply(ast.literal_eval)\n",
    "network_estreams[\"nested_catchments\"] = network_estreams[\"nested_catchments\"].apply(ast.literal_eval)\n",
    "\n",
    "network_estreams.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6445ae63",
   "metadata": {},
   "source": [
    "- Duplicated suspects deletion\n",
    "    - At this part, when there is a duplicated suspect in our catchments list we keep only the catchemnt with the longest time-series.\n",
    "    - For example, FR001479 has 23 years of measurements, from  1969 to 1991, and has two duplicated suspects: [FR001477, FR001478].\n",
    "    - After our filter, we aim to keep only FR001479 in our final list, since it is the one with the longest number of measurements from the three. \n",
    "    - Eventually we mitigate the number of potential duplicates in our time-series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e4db0f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we make a copy of the original network metadata \n",
    "network_estreams_filtered = network_estreams.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "420ce646",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Filter rows where `duplicated_suspect` is not NaN\n",
    "filtered_df = network_estreams_filtered[network_estreams_filtered['duplicated_suspect'].notna()]\n",
    "\n",
    "# Step 2: Create a dictionary to store the maximum `num_years` for each group (current row and corresponding row(s))\n",
    "max_num_years_dict = {}\n",
    "processed_indices = set()  # Set to keep track of processed indices\n",
    "\n",
    "# Iterate through each row in the filtered DataFrame\n",
    "for index, row in filtered_df.iterrows():\n",
    "    # Check if the current index has already been processed\n",
    "    if index in processed_indices:\n",
    "        continue  # Skip processing this row\n",
    "\n",
    "    # Get the `duplicated_suspect` values, assuming it might be a list or a single string index\n",
    "    duplicate_indices = row['duplicated_suspect']\n",
    "\n",
    "    # If `duplicated_suspect` is a string, convert it to a list of strings and strip whitespace\n",
    "    if isinstance(duplicate_indices, str):\n",
    "        duplicate_indices = [dup.strip() for dup in duplicate_indices.split(',')]  # Split and strip whitespace\n",
    "\n",
    "    # Initialize the maximum `num_years` as the current row's `num_years`\n",
    "    max_num_years = row['num_years']\n",
    "    max_index = index  # Start with the current index as the max index\n",
    "\n",
    "    # Compare `num_years` of the current row with each duplicate index\n",
    "    for dup_index in duplicate_indices:\n",
    "        # Check if the duplicate index has already been processed\n",
    "        if dup_index in processed_indices:\n",
    "            continue  # Skip processing this duplicate index\n",
    "\n",
    "        try:\n",
    "            # Get `num_years` of the duplicate row\n",
    "            num_years_duplicate = network_estreams_filtered.loc[dup_index, 'num_years']\n",
    "\n",
    "            # Compare the `num_years` and update max values if necessary\n",
    "            if num_years_duplicate > max_num_years:\n",
    "                max_num_years = num_years_duplicate\n",
    "                max_index = dup_index  # Update max index\n",
    "        except KeyError:\n",
    "            # Handle KeyError if the duplicate index is not found in the DataFrame\n",
    "            continue\n",
    "\n",
    "    # Store the maximum `num_years` and corresponding index in the dictionary\n",
    "    max_num_years_dict[max_index] = max_num_years\n",
    "\n",
    "    # Add the indices to the processed set\n",
    "    processed_indices.add(index)\n",
    "    for dup_index in duplicate_indices:\n",
    "        processed_indices.add(dup_index)\n",
    "\n",
    "# Step 3: Filter the DataFrame to keep only the rows with the indices in max_num_years_dict keys\n",
    "result_df = network_estreams_filtered.loc[list(max_num_years_dict.keys())]\n",
    "\n",
    "# Step 4: Get the indices of the rows in `result_df`\n",
    "result_df_indices = set(result_df.index)\n",
    "\n",
    "# Step 5: Get the indices of rows without duplicates (where `duplicated_suspect` is NaN)\n",
    "no_duplicates_indices = set(network_estreams_filtered[network_estreams_filtered['duplicated_suspect'].isna()].index)\n",
    "\n",
    "# Step 6: Combine the indices from `result_df` and rows without duplicates\n",
    "indices_to_keep = list(result_df_indices.union(no_duplicates_indices))\n",
    "\n",
    "# Step 7: Filter `network_estreams_filtered` using the combined indices\n",
    "network_estreams_filtered = network_estreams_filtered.loc[indices_to_keep]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9683854",
   "metadata": {},
   "source": [
    "### - Check the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ba6f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "network_estreams_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d409cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the number of \"unique\" gauges\n",
    "len(network_estreams_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d56be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export the metadata list\n",
    "network_estreams_filtered.to_csv(\"results/extras/estreams_attributes.csv\", encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "801ba072",
   "metadata": {},
   "source": [
    "## End"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
