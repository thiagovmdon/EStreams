{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46e94d6f",
   "metadata": {},
   "source": [
    "# Concatenation of new catchments\n",
    "\n",
    "\n",
    "Author: Thiago Nascimento (thiago.nascimento@eawag.ch)\n",
    "\n",
    "This notebook is part of the EStreams publication and is used concateante the series of catchment attributes aggregated into the original datasets. \n",
    "\n",
    "\n",
    "* Note that this code enables not only the replicability of the current database but also the extrapolation to new catchment areas. \n",
    "* Additionally, the user should download and insert the original raw-data in the folder of the same name prior to run this code. \n",
    "* The original third-party data used were not made avaialble in this repository due to redistribution and storage-space reasons.  \n",
    "\n",
    "## Requirements\n",
    "**Python:**\n",
    "\n",
    "* Python>=3.6\n",
    "* Jupyter\n",
    "* geopandas=0.10.2\n",
    "* numpy\n",
    "* os\n",
    "* pandas\n",
    "* tqdm\n",
    "* warnings\n",
    "\n",
    "Check the Github repository for an environment.yml (for conda environments) or requirements.txt (pip) file.\n",
    "\n",
    "**Files:**\n",
    "\n",
    "* data/concatenation_new_basins/old_data/*.csv\n",
    "* data/concatenation_new_basins/new_data/*.csv\n",
    "\n",
    "**Directory:**\n",
    "\n",
    "* Clone the GitHub directory locally\n",
    "* Place any third-data variables in their respective directory.\n",
    "* ONLY update the \"PATH\" variable in the section \"Configurations\", with their relative path to the EStreams directory. \n",
    "\n",
    "## Observations\n",
    "\n",
    "* This notebook assumes that all the new catchments have their attributes exported and stored correctly alongside with the \"old\" dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f7648ab",
   "metadata": {},
   "source": [
    "# Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f6e053",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tqdm as tqdm\n",
    "import os\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b31c9d67",
   "metadata": {},
   "source": [
    "# Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13078a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only editable variables\n",
    "# Relative path to your local directory\n",
    "PATH = \"../../..\"\n",
    "\n",
    "# Suppress all warnings:\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f5fc4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Non-editable variables:\n",
    "PATH_OUTPUT = \"results/\"\n",
    "path_old_files = 'data/concatenation_new_basins/old_files/'\n",
    "path_new_files = 'data/concatenation_new_basins/new_files/'\n",
    "\n",
    "# Set the directory:\n",
    "os.chdir(PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4909fe84",
   "metadata": {},
   "source": [
    "# Import data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4644d225",
   "metadata": {},
   "source": [
    "## Static attributes\n",
    "* Here we automatize the concatenation of the new dataset into the old ones.\n",
    "* Note that the concatenation is done in pairs (old and new)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f41d0ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Static datasets names:\n",
    "datasets_static = ['estreams_geology_attributes.csv',\n",
    "    'estreams_hydrology_attributes.csv',\n",
    "    'estreams_landcover_attributes.csv',\n",
    "    'estreams_meteorology_density.csv',\n",
    "    'estreams_snowcover_attributes.csv',\n",
    "    'estreams_soil_attributes.csv',\n",
    "    'estreams_terrain_attributes.csv',\n",
    "    'estreams_vegetation_attributes.csv']\n",
    "\n",
    "# Output filename options:\n",
    "datasets_static_output_options = {\n",
    "    'estreams_geology_attributes.csv': PATH_OUTPUT+\"/staticattributes/estreams_geology_attributes.csv\",\n",
    "    'estreams_hydrology_attributes.csv': PATH_OUTPUT+\"/staticattributes/estreams_hydrology_attributes.csv\",\n",
    "    'estreams_landcover_attributes.csv': PATH_OUTPUT+\"/staticattributes/estreams_landcover_attributes.csv\",\n",
    "    'estreams_meteorology_density.csv': PATH_OUTPUT+\"/staticattributes/estreams_meteorology_density.csv\",\n",
    "    'estreams_snowcover_attributes.csv': PATH_OUTPUT+\"/staticattributes/estreams_snowcover_attributes.csv\",\n",
    "    'estreams_soil_attributes.csv': PATH_OUTPUT+\"/staticattributes/estreams_soil_attributes.csv\",\n",
    "    'estreams_terrain_attributes.csv': PATH_OUTPUT+\"/staticattributes/estreams_terrain_attributes.csv\",\n",
    "    'estreams_vegetation_attributes.csv': PATH_OUTPUT+\"/staticattributes/estreams_vegetation_attributes.csv\"\n",
    "    }\n",
    "\n",
    "for chosen_dataset in tqdm.tqdm(datasets_static):\n",
    "    \n",
    "    # If the file is not in the folder (or stored with a different name), the run will skip this dataset. \n",
    "    try: \n",
    "        # Get the old, new and output pathnames\n",
    "        csv_file_old = path_old_files+chosen_dataset\n",
    "        csv_file_new = path_new_files+chosen_dataset\n",
    "        csv_output = datasets_static_output_options.get(chosen_dataset)\n",
    "        \n",
    "        # Read the old and new datasets:\n",
    "        estreams_old = pd.read_csv(csv_file_old, index_col=0)\n",
    "        estreams_new = pd.read_csv(csv_file_new, index_col=0)\n",
    "        \n",
    "        # Concatenate and sort it by index\n",
    "        estreams_concatenated = pd.concat([estreams_old, estreams_new], axis=0)\n",
    "        estreams_concatenated = estreams_concatenated.sort_index(axis=0)\n",
    "        \n",
    "        # Save the data:\n",
    "        estreams_concatenated.to_csv(csv_output)\n",
    "        \n",
    "        # Prints usefull to visualize what was done:\n",
    "        print(\"Dataset:\", chosen_dataset)\n",
    "        print(\"Number of catchments in the old file:\", len(estreams_old))\n",
    "        print(\"Number of catchments in the new file:\", len(estreams_new))\n",
    "        \n",
    "    except:\n",
    "        print(\"The dataset\", chosen_dataset, \"is not avaialble in the folders.\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a4007c",
   "metadata": {},
   "source": [
    "## Temporal attributes\n",
    "* Here we automatize the concatenation of the new dataset into the old ones.\n",
    "* Note that the concatenation is done in pairs (old and new).\n",
    "* The meteorological time-series are not considered at this part. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9855a557",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temporal datasets names:\n",
    "datasets_temporal = [\n",
    "    'estreams_irrigation_yearly.csv',\n",
    "    'estreams_LAI_monhtly.csv',\n",
    "    'estreams_LAI_yearly.csv',\n",
    "    'estreams_NDVI_monhtly.csv',\n",
    "    'estreams_NDVI_yearly.csv',\n",
    "    'estreams_snowcover_monhtly.csv',\n",
    "    'estreams_snowcover_yearly.csv']\n",
    "\n",
    "# Output filename options:\n",
    "datasets_temporal_output_options = {\n",
    "    'estreams_irrigation_yearly.csv': PATH_OUTPUT+\"/timeseries/irrigation/estreams_irrigation_yearly.csv\",\n",
    "    'estreams_LAI_monhtly.csv': PATH_OUTPUT+\"/timeseries/vegetationindices/estreams_LAI_monhtly.csv\",\n",
    "    'estreams_LAI_yearly.csv': PATH_OUTPUT+\"/timeseries/vegetationindices/estreams_LAI_yearly.csv\",\n",
    "    'estreams_NDVI_monhtly.csv': PATH_OUTPUT+\"/timeseries/vegetationindices/estreams_NDVI_monhtly.csv\",\n",
    "    'estreams_NDVI_yearly.csv': PATH_OUTPUT+\"/timeseries/vegetationindices/estreams_NDVI_yearly.csv\",\n",
    "    'estreams_snowcover_monhtly.csv': PATH_OUTPUT+\"/timeseries/snowcover/estreams_snowcover_monhtly.csv\",\n",
    "    'estreams_snowcover_yearly.csv': PATH_OUTPUT+\"/timeseries/snowcover/estreams_snowcover_yearly.csv\",\n",
    "    }\n",
    "\n",
    "for chosen_dataset in tqdm.tqdm(datasets_temporal):\n",
    "    \n",
    "    # If the file is not in the folder (or stored with a different name), the run will skip this dataset. \n",
    "    try: \n",
    "        # Get the old, new and output pathnames\n",
    "        csv_file_old = path_old_files+chosen_dataset\n",
    "        csv_file_new = path_new_files+chosen_dataset\n",
    "        csv_output = datasets_temporal_output_options.get(chosen_dataset)\n",
    "        \n",
    "        # Read the old and new datasets:\n",
    "        estreams_old = pd.read_csv(csv_file_old, index_col=0)\n",
    "        estreams_new = pd.read_csv(csv_file_new, index_col=0)\n",
    "        \n",
    "        # Concatenate and sort it by index\n",
    "        estreams_concatenated = pd.concat([estreams_old, estreams_new], axis=1)\n",
    "        estreams_concatenated = estreams_concatenated.sort_index(axis=1)\n",
    "        \n",
    "        # Save the data:\n",
    "        estreams_concatenated.to_csv(csv_output)\n",
    "        \n",
    "        # Prints usefull to visualize what was done:\n",
    "        print(\"Dataset:\", chosen_dataset)\n",
    "        print(\"Number of catchments in the old file:\", estreams_old.shape[1])\n",
    "        print(\"Number of catchments in the new file:\", estreams_new.shape[1])\n",
    "        \n",
    "    except:\n",
    "        print(\"The dataset\", chosen_dataset, \"is not avaialble in the folders.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a08b6940",
   "metadata": {},
   "source": [
    "## Meteorological csv-data (PET, P and T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fb135a6",
   "metadata": {},
   "source": [
    "### PET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a4b625d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the old and new datasets:\n",
    "estreams_old = pd.read_csv(path_old_files+\"estreams_meteorology_pet.csv\", index_col=0)\n",
    "estreams_new = pd.read_csv(path_new_files+\"estreams_meteorology_pet.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc001905",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we set the index to datetime (avoid problems during concatenation):\n",
    "try:\n",
    "    estreams_old.index = pd.to_datetime(estreams_old.index)\n",
    "except:\n",
    "    estreams_old.index = pd.to_datetime(estreams_old.index, format='%d.%m.%Y')\n",
    "try:\n",
    "    estreams_new.index = pd.to_datetime(estreams_new.index)\n",
    "except:\n",
    "    estreams_new.index = pd.to_datetime(estreams_new.index, format='%d.%m.%Y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "894578c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate and sort it by index\n",
    "estreams_concatenated = pd.concat([estreams_old, estreams_new], axis=1, ignore_index=False)\n",
    "estreams_concatenated = estreams_concatenated.sort_index(axis=1)\n",
    "estreams_concatenated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea4df24f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the data:\n",
    "estreams_concatenated.to_csv(\"results/timeseries/meteorology/estreams_meteorology_pet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ca9bfde",
   "metadata": {},
   "source": [
    "### P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdff502c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the old and new datasets:\n",
    "estreams_old = pd.read_csv(path_old_files+\"estreams_meteorology_precipitation.csv\", index_col=0)\n",
    "estreams_new = pd.read_csv(path_new_files+\"estreams_meteorology_precipitation.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f9a585b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we set the index to datetime (avoid problems during concatenation):\n",
    "try:\n",
    "    estreams_old.index = pd.to_datetime(estreams_old.index)\n",
    "except:\n",
    "    estreams_old.index = pd.to_datetime(estreams_old.index, format='%d.%m.%Y')\n",
    "try:\n",
    "    estreams_new.index = pd.to_datetime(estreams_new.index)\n",
    "except:\n",
    "    estreams_new.index = pd.to_datetime(estreams_new.index, format='%d.%m.%Y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf44e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate and sort it by index\n",
    "estreams_concatenated = pd.concat([estreams_old, estreams_new], axis=1, ignore_index=False)\n",
    "estreams_concatenated = estreams_concatenated.sort_index(axis=1)\n",
    "estreams_concatenated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "977eb085",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the data:\n",
    "estreams_concatenated.to_csv(\"results/timeseries/meteorology/estreams_meteorology_precipitation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10bdd987",
   "metadata": {},
   "source": [
    "### T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01cb6227",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the old and new datasets:\n",
    "estreams_old = pd.read_csv(path_old_files+\"estreams_meteorology_temperature.csv\", index_col=0)\n",
    "estreams_new = pd.read_csv(path_new_files+\"estreams_meteorology_temperature.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bcc7db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we set the index to datetime (avoid problems during concatenation):\n",
    "try:\n",
    "    estreams_old.index = pd.to_datetime(estreams_old.index)\n",
    "except:\n",
    "    estreams_old.index = pd.to_datetime(estreams_old.index, format='%d.%m.%Y')\n",
    "try:\n",
    "    estreams_new.index = pd.to_datetime(estreams_new.index)\n",
    "except:\n",
    "    estreams_new.index = pd.to_datetime(estreams_new.index, format='%d.%m.%Y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5a89d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate and sort it by index\n",
    "estreams_concatenated = pd.concat([estreams_old, estreams_new], axis=1, ignore_index=False)\n",
    "estreams_concatenated = estreams_concatenated.sort_index(axis=1)\n",
    "estreams_concatenated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38feff13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the data:\n",
    "estreams_concatenated.to_csv(\"results/timeseries/meteorology/estreams_meteorology_temperature\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cfa5cd2",
   "metadata": {},
   "source": [
    "# End"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
